<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://kusionstack.io/blog</id>
    <title>Evolve your Internal Developer Platform with KusionStack Blog</title>
    <updated>2022-12-12T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://kusionstack.io/blog"/>
    <subtitle>Evolve your Internal Developer Platform with KusionStack Blog</subtitle>
    <icon>https://kusionstack.io/img/kusionstack-icon-square.png</icon>
    <entry>
        <title type="html"><![CDATA[How to scale operation in Post Cloud Native Era?]]></title>
        <id>2022-post-cloud-native-era-operation</id>
        <link href="https://kusionstack.io/blog/2022-post-cloud-native-era-operation"/>
        <updated>2022-12-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[It has been more than eight years since the first commit of Kubernetes, and the cloud-native technology represented by it is no longer new but a "default option" for modern applications. The services that modern applications rely on are far more than just Kubernetes. A slightly more complex application often uses heterogeneous infrastructures such as Kubernetes ecological cloud-native technology, IaaS cloud services, and internal self-built systems. Multi-clouds and hybrid clouds are also usually required. We have entered the "Post cloud-native Era", and the operation tools only for Kubernetes can no longer meet our demands.]]></summary>
        <content type="html"><![CDATA[<h1></h1><h1>Post Cloud Native Era</h1><p>It has been more than eight years since the first commit of Kubernetes, and the cloud-native technology represented by it is no longer new but a "default option" for modern applications. The services that modern applications rely on are far more than just Kubernetes. A slightly more complex application often uses heterogeneous infrastructures such as Kubernetes ecological cloud-native technology, IaaS cloud services, and internal self-built systems. Multi-clouds and hybrid clouds are also usually required. We have entered the "Post cloud-native Era", and the operation tools only for Kubernetes can no longer meet our demands.</p><p align="center"><img src="/img/blog/2022-12-12-post-cloud-native-era-operation/modern-app.png" width="60%"></p><p>What's more complicated is that, within the enterprise, different teams generally maintain these services. A large-scale operation requires the cooperation of members of multiple teams. However, there is a lack of efficient communication and collaborative approach among App Dev, Platform Dev, and SRE teams. The complexity of technology and inefficient teamwork has exponentially increased the difficulty of large-scale operation and maintenance in the "Post cloud-native Era".</p><h1>The problem of large-scale operations has always existed</h1><p>The large-scale operation of complex heterogeneous infrastructure is not a unique problem in the post cloud-native era. It has always been a problem since the birth of distributed systems, but it has become more difficult in the post cloud-native era. The industry proposed the DevOps concept more than ten years ago. Countless companies have built their DevOps platforms based on this concept, hoping to solve this problem, but the actual implementation process is often unsatisfactory. How to cooperate between the Dev team and the Ops team? How are responsibilities divided? How can a platform team of dozens of people support the operation demands of tens of thousands of engineers? The underlying infrastructure is complex and diverse, and capabilities change with each passing day. How to quickly help front-line Devs get technological advantages? These problems still need to be resolved. Recently, some people have suggested that DevOps is dead and Platform Engineering is the future. Regardless of the concept definition, whether DevOps or Platform Engineering, they are essentially different concepts under the same proposition of large-scale operation in enterprises. What we need more is a solution that conforms to the trend of technological development and can solve current problems.</p><h1>Legacy architecture is no longer applicable</h1><p>In traditional operation and maintenance thinking, the solution to the above problems is generally to build a PaaS platform, such as our early AntGroup PaaS platform, a web console with a UI interface. Users (usually App Dev or SRE) can accomplish operations such as deploying, restarting, scaling, and so on through UI interactions. In terms of technical implementation, the system mainly contains three parts, a frontend system that provides user interactions regarded as the system entrance; a backend system in the middle that connects to various infrastructures; the bottom layer is the APIs of multiple infrastructures. This architecture has been running for nearly ten years and has been running very well. It has a user-friendly interface and can shield the complexity of the infrastructure, and the responsibilities of each team are clearly defined. However, in the post cloud-native era, this architecture is no longer applicable, exposing two fatal flaws, <strong>"manpower-consuming" and "time-consuming"</strong>.</p><p align="center"><img src="/img/blog/2022-12-12-post-cloud-native-era-operation/classic.png" width="30%"></p><p>To give a typical example, the network team has developed a new load balance algorithm for its Loadbalancer, which needs to be provided to users. Under the above architecture, the entire workflow looks like this:</p><ol><li>The network team developed the new load balance algorithm and provided APIs</li><li>The PaaS backend coding with the underlying APIs to interconnect various infrastructures and shield complexity. Abstract higher-level APIs for users</li><li>The PaaS frontend modifies the UI according to the new feature and uses the backend APIs to provide the new load balance algorithm to end users</li></ol><p>There is a problem here. Even a tiny feature requires the PaaS backend and frontend to modify the code. The process will take a week to go online at the fastest, and the more infrastructure teams involved, the lower the efficiency. It was not a problem ten years ago but is a big problem today. A post cloud-native era modern application relying on three cloud-native technologies (Kubernetes + Istio + Prometheus), two cloud services ( Loadbalancer + Database), and a self-built internal service has already become prevalent, and complex applications will rely on more. If every infrastructure is hard-coded by the PaaS team, expanding the PaaS team by ten times will not be enough.</p><p align="center"><img src="/img/blog/2022-12-12-post-cloud-native-era-operation/circle.png" width="50%"></p><p>After talking about "manpower-consuming", let's look at the problem of "time-consuming". A minor feature in the above example requires two cross-team collaborations. The first collaboration is between the infrastructure team and the PaaS backend team, and the second is between the PaaS backend team and the PaaS frontend team. Teamwork is a complicated problem, sometimes more complicated than the technology itself. If you want to accomplish a large-scale operation with 100 applications at a time, how many teams do you need to communicate and collaborate with? How much time will it take? Without suitable coordination mechanisms, this becomes an impossible task.</p><h1>Explore and practice</h1><p>We have been exploring within Ant Group for nearly two years. We have practiced common tools such as kustomize, helm, argoCD, and Terraform and even developed some auxiliary systems for some tools, but the results are unsatisfactory. Some of these tools are too limited to the Kubernetes ecosystem to operate other types of infrastructure. The others support heterogeneous infrastructure but are not friendly to the Kubernetes ecosystem and cannot take advantage of cloud-native technologies. More importantly, upgrading operation tools has hardly improved teamwork efficiency, and we need a more systematic solution.
Going back to the question itself, we propose two ideas for the problems of "manpower-consuming" and "time-consuming":</p><ol><li>Is it possible for App Dev to use various interconnected infrastructures efficiently self-service instead of making PaaS a transfer?</li><li>Is it possible to build a centralized collaboration platform using technical means to regulate everyone's behavior and communicate in a standardized manner?</li></ol><p>From a technical point of view, the PaaS platform must provide flexible toolchains and workflows. All capabilities of the infrastructure are exposed in a modular manner. App Dev combines and orchestrates these platforms' basic capabilities to solve their problems, and the process does not require the participation of the platform team. All teams involved in the whole process use a unified language and interface to communicate without manual involvement in the entire process.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="our-practice">Our practice<a class="hash-link" href="#our-practice" title="Direct link to heading">​</a></h2><p><img src="https://raw.githubusercontent.com/KusionStack/kusion/main/docs/arch.png"></p><p>After nearly two years of exploration and practice on the AntGroup's internal PaaS platform, we precipitated a complete end-to-end solution named <a href="https://github.com/KusionStack/kusion" target="_blank" rel="noopener noreferrer">KusionStack</a>, which is open source now. KusionStack is designed to solve the traditional PaaS "manpower-consuming consuming" and "time-consuming" problems from the perspective of unified heterogeneous infrastructure operation and team collaboration. The whole system mainly contains three parts:</p><ol><li><a href="https://github.com/KusionStack/konfig" target="_blank" rel="noopener noreferrer">Konfig</a>: It is a monorepo acting as a centralized platform for multi-team collaboration, storing the operation intentions of each team.</li><li><a href="https://github.com/KusionStack/KCLVM" target="_blank" rel="noopener noreferrer">KCL</a>: An self-developed configuration DSL. It is a tool for communication between all teams.</li><li><a href="https://github.com/KusionStack/kusion" target="_blank" rel="noopener noreferrer">Kusion</a>: KusionStack's engine, responsible for all operations</li></ol><p>Platform Dev defines the basic capability model through KCL, and App Dev reuses these predefined capabilities in the application configuration model (AppConfig) through language features such as import and mixin. Users can quickly describe operation intentions in Konfig. AppConfig is a well-designed model that only exposes the attributes that App Dev needs to care about, shielding the complexity of the infrastructure.</p><p>Never underestimate the professionalism and complexity of infrastructures. Even Kubernetes, which has become the standard of cloud-native technology, still has a high threshold for ordinary users. A Kubernetes Deployment has dozens of fields, let alone custom labels and annotations. Ordinary users cannot understand them all. In other words,  AppDev should not understand Kubernetes, all they need is release, and they do not even need to care whether the underlying infrastructure is Kubernetes.</p><p>AppConfig will generate multiple heterogeneous infrastructure resources after compilation and transfer these resources to the KusionStack engine through CI, CLI, GUI, etc. The engine is the core of KusionStack, responsible for all operations, and makes the operation intentions take effect on the infrastructure. It operates heterogeneous infrastructure in a unified way and performs a series of procedures on these resources, such as verification, arrangement, preview, validation, observation, and health check.</p><p>It is worth mentioning that the whole process is very friendly to  Kubernetes resources. Due to the Kubernetes reconciliation mechanism, the success of the apply command does not mean that resources are available. Applications need to wait for resources to be reconciled successfully. If the reconciliation fails, we need to log in to the cluster and check the specific error message through commands like get, describe, and log. The whole process is very cumbersome. We have simplified these operations through technical means and showed important messages during the reconciliation in a user-friendly way. The animation below is a simple example. After the command is invoked, you can clearly see the reconciliation process of all resources and their associated resources until the resources are actually available.</p><p align="center"><img src="/img/docs/user_docs/getting-started/apply.gif"></p><p>The whole system has the following characteristics</p><ol><li>Application-centric<ul><li>Comprehensive application configuration management, including all application-related configurations such as computing, network, and storage</li><li>Application life cycle management, from the first line of configuration code to production availability</li></ul></li><li>Unified operation of heterogeneous infrastructure for applications in the post cloud-native era<ul><li>Kubernetes-friendly workflow, providing high-level capabilities such as observability and health checks for Kubernetes resources and releasing the bonuses of cloud-native technologies</li><li>Reuse Terraform ecology, unified workflow operation, and maintenance multi-runtime resources like Kubernetes and Terraform </li></ul></li><li>Large-scale collaborative platform<ul><li>Flexible workflow, users can use the basic capabilities of the platform to solve their problems by combining and arranging themselves</li><li>Separate the focus of App Dev and Platform Dev. The infrastructure feature iteration does not require platform intervention and can be directly used by App Dev</li><li>Pure client-side solution, "shift-left" risks, problems can be detected as early as possible</li></ul></li></ol><h1>It is just the beginning</h1><p>After nearly two years of exploration, this system has been widely used in AntGroup multi-cloud application delivery, computing and data infrastructure delivery, database operation, and other business fields. Currently, 400+ developers have directly participated in Konfig monorepo contribution; a total of nearly 800K Commits, most of which are machine automation code modifications; an average of 1K pipeline task execution, and about 10K KCL compilation execution per day. After Konfig monorepo compilation, 3M+ lines of YAML text can be generated.</p><p>However, all this has just begun, and the post cloud-native era has just arrived. Our purpose of open-sourcing this system is also to invite all parties in the industry to build a solution that can truly solve the current large-scale operation of enterprises. The AntGroup's PaaS team still has a lot of technology precipitation that has been verified in internal scenarios, and they will be open sourced in the future. We are far from enough, and we sincerely invite everyone to play together.</p><h1>Ref</h1><p>Github: Welcome to give a Star⭐️ </p><ul><li><a href="https://github.com/KusionStack/kusion" target="_blank" rel="noopener noreferrer">https://github.com/KusionStack/kusion</a></li><li><a href="https://github.com/KusionStack/KCLVM" target="_blank" rel="noopener noreferrer">https://github.com/KusionStack/KCLVM</a></li><li><a href="https://github.com/KusionStack/konfig" target="_blank" rel="noopener noreferrer">https://github.com/KusionStack/konfig</a></li></ul><p>Website：<a href="https://kusionstack.io/" target="_blank" rel="noopener noreferrer">https://kusionstack.io</a></p><p>PPT：<a href="https://kusionstack.io/blog/2022-kusionstack-application-scale-operation-solution-in-the-post-cloud-native-era" target="_blank" rel="noopener noreferrer">KusionStack: Application Scale Operation Solution in the "Post CloudNative" Era</a></p>]]></content>
        <author>
            <name>Dayuan Li</name>
        </author>
        <category label="KusionStack" term="KusionStack"/>
        <category label="Kusion" term="Kusion"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[KusionStack:Application Scale Operation Solution in the "Post CloudNative" Era]]></title>
        <id>2022-kusionstack-application-scale-operation-solution-in-the-post-cloudnative-era</id>
        <link href="https://kusionstack.io/blog/2022-kusionstack-application-scale-operation-solution-in-the-post-cloudnative-era"/>
        <updated>2022-11-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[KusionStack: Application Scale Operation Solution in the "Post CloudNative" Era]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/KusionStack/community/raw/main/2022/talkgo/kusionstack-application-scale-operation-solution-in-the-post-cloudnative-era.pdf" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="KusionStack: Application Scale Operation Solution in the &amp;quot;Post CloudNative&amp;quot; Era" src="/assets/images/kusionstack-application-scale-operation-solution-in-the-post-cloudnative-era-f1f693d4bb2d7609a6434ad99d790296.png" width="1100" height="619"></a></p><p><a href="https://github.com/KusionStack/community/raw/main/2022/talkgo/kusionstack-application-scale-operation-solution-in-the-post-cloudnative-era.pdf" target="_blank" rel="noopener noreferrer">Download PDF</a></p>]]></content>
        <author>
            <name>Dayuan Li</name>
        </author>
        <category label="KusionStack" term="KusionStack"/>
        <category label="Kusion" term="Kusion"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[KusionStack Origin, present and future]]></title>
        <id>2022-origin-present-and-future</id>
        <link href="https://kusionstack.io/blog/2022-origin-present-and-future"/>
        <updated>2022-09-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[KusionStack Origin, present and future]]></summary>
        <content type="html"><![CDATA[<p><a target="_blank" href="/assets/files/KusionStack-origin-present-and-future-157e81f563bafa9abfae441c0e651896.pdf"><img loading="lazy" alt="KusionStack Origin, present and future" src="/assets/images/KusionStack-origin-present-and-future-c678397b18a35d71fe4661a12eb16666.png" width="2636" height="1476"></a></p><p><a href="https://kusionstack.io/talks/KusionStack-origin-present-and-future.pdf" target="_blank" rel="noopener noreferrer">Download PDF</a></p>]]></content>
        <author>
            <name>Xiaodong, Duo</name>
        </author>
        <category label="KusionStack" term="KusionStack"/>
        <category label="Kusion" term="Kusion"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[What We Learned From Large-scale Platform Engineering Practices]]></title>
        <id>2022-learn-from-scale-practice</id>
        <link href="https://kusionstack.io/blog/2022-learn-from-scale-practice"/>
        <updated>2022-09-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Abstract: This blog attempts to talk about the challenges and best practices in the process of large-scale platform engineering from the perspectives of engineering, programing language, divide-and-conquer, modeling, automation, and collaborative culture. Hopefully, by sharing the concepts and practices of our platform engineering with more companies and teams, we can make some interesting changes happen together.]]></summary>
        <content type="html"><![CDATA[<p><strong>Abstract:</strong> This blog attempts to talk about the challenges and best practices in the process of large-scale platform engineering from the perspectives of engineering, programing language, divide-and-conquer, modeling, automation, and collaborative culture. Hopefully, by sharing the concepts and practices of our platform engineering with more companies and teams, we can make some interesting changes happen together.</p><p>This blog is based on the platform engineering and automation practice of <a href="https://kusionstack.io/docs/user_docs/intro/kusion-intro" target="_blank" rel="noopener noreferrer">KusionStack</a> in Ant Group.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-platform-engineering-making-enterprise-devops-happen">1. Platform Engineering: Making Enterprise DevOps Happen<a class="hash-link" href="#1-platform-engineering-making-enterprise-devops-happen" title="Direct link to heading">​</a></h2><p>The concept of DevOps was proposed more than 10 years ago. From KVM to container to the cloud-native era, a large number of enterprises have invested in the DevOps wave to solve the dilemma of internal large-scale operation efficiency and platform construction efficiency. Most of them have fallen into some kind of Anti-Pattern based on an inappropriate understanding of DevOps, while some companies have explored their golden paths. I have experienced Anti-Patterns as shown in the figure below, where the Dev and Ops teams go their separate ways, or simply force the Dev team to handle Ops work independently. More typical classifications can be found <a href="https://web.devopstopologies.com/#anti-types" target="_blank" rel="noopener noreferrer">here</a>.</p><p><img loading="lazy" src="/assets/images/devops-anti-pattern-a9159a518bb8f608b79a505dad7f2aaa.png" width="988" height="410"></p><p>There are various reasons for difficulties achieving DevOps at scale within the enterprise, especially for companies that maintain their infrastructure within the enterprise while adopting technology platforms on the cloud. Among which, the following situations are quite common:</p><ul><li>The dev team and the ops team are often isolated at work and cannot reach a consensus due to team silos, lack of leaders’ insight, etc.</li><li>The dev team leaders underestimate the professionalism, complexity and rapid changes of platform and infrastructure technology, operation and reliability work so that they push application developers to become experts with a simple DevOps understanding</li><li>The leaders established a DevOps team, but became just middle executors, failing to make the Dev and Ops teams move forward and work closely together</li><li>The platform dev team responds insufficiently to the business complexity due to the scale as well as to the tech complexity as a result of technological evolution, thus cannot provide effective support to application developers</li><li>...</li></ul><p>Unlike small teams working on cloud-hosted Infra services and DevOps-as-a-Service products, medium and large enterprises often need to establish an appropriate DevOps system based on their team structure and culture. Based on the successful cases, whether for Meta Dev team fully assumes the Ops function, or Google introduces the SRE team as the middle layer, <a href="https://platformengineering.org/blog/what-is-platform-engineering" target="_blank" rel="noopener noreferrer">Platform Engineering</a> plays a very important role. Platform engineering aims to help enterprises build a self-service operation system for application developers, and tries to solve the following key problems through engineering tech means and workflows:</p><ul><li>Design a reasonable abstraction to help application developers reduce the burden of cognizing the delivery, operation, and reliability based on Infra, platform and other technologies</li><li>Provide application developers with a unified working interface and space to avoid falling into fragmented product interfaces and complex workflows</li><li>Help developers carry out work quickly based on <a href="https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/" target="_blank" rel="noopener noreferrer">internal engineering platform</a> through effective workflow and recommended path</li><li>Help developers manage the application life cycle through self-service CI, CD, CDRA products</li><li>Help the platform dev team to open fundamental platform capabilities in a simple, efficient and consistent manner</li><li>Create a culture of collaboration and sharing through training, propagation, operations, etc.</li></ul><p>In fact, not everyone should be or could be an expert in this specific field, which turns out to be particularly hard as well. Even the experts from the platform technology team themselves are usually good at their own professional fields, especially nowadays confronting the wide adoption of cloud-native concepts and technologies, hundreds of thousands of application configurations brought by a large number of highly open and configurable platform technologies and the business complexity of the PaaS field as well as the requirements for high stability and unified governance. The purpose of platform engineering just lies in allowing application developers to participate in such large-scale DevOps work as easily and painlessly as possible. In Ant Group's practice, we tend to the following cooperative state, which is closer to Google's best practices in team structure and work mode. Platform developers and SREs become "Enablers" to support application developers to complete dev, delivery and operation in self-service. At the same time, the work results of application developers making applications deliverable and operational also become the basis for the ops team to handle application ops work. The SRE, application dev, and ops team periodically feedback the problems and pain points in the work process to the platform dev team to form a positive cycle.</p><p><img loading="lazy" src="/assets/images/devops-cycle-8923aa81a904e40ee30ede0c0d4c16b2.png" width="694" height="666"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-domain-language-a-pole-of-the-engineering-thought">2. Domain Language: A Pole of the Engineering Thought<a class="hash-link" href="#2-domain-language-a-pole-of-the-engineering-thought" title="Direct link to heading">​</a></h2><p>Compared with a domain language there's no better way for open, self-service, domain-oriented business problem definitions, as well as meeting the enterprise's internal requirements of automation, low-security-risk, low noise, and easy governance. Just as there are staves for recording music, and time-series databases for storing time-series data, within the problem domain of platform engineering, a set of configuration and policy languages ​​are created to write and manage configurations and policies at scale. Different from high-level general purpose languages ​​with mixed paradigms and engineering capabilities, the core logic of such domain languages ​​is to solve the near-infinite variation and complexity of domain problems with a convergent and limited set of syntax and semantics, and to integrate the ideas and methods of large-scale complex configuration and policies writing into language features.</p><p>In the platform engineering practice of Ant Group, we have strengthened the client-side working mode. We write and maintain the models, orchestration, constraints and policies around the application ops life cycle in the shared codebase <a href="https://github.com/KusionStack/konfig" target="_blank" rel="noopener noreferrer">Konfig</a> through the record and functional language <a href="https://github.com/KusionStack/KCLVM" target="_blank" rel="noopener noreferrer">KCL</a>. KCL is a static and strongly typed language for application developers with programming ability, and provides the writing experience of a modern high-level language with limited functionality around domain purposes. Under such practices, KCL is not a language just for writing K-V pairs, but a language for platform engineering development. Application developers, SREs, and platform developers conduct dev collaboratively based on Konfig. They write configurations, and <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#schema" target="_blank" rel="noopener noreferrer">schema</a> abstractions, <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#function" target="_blank" rel="noopener noreferrer">functions</a>, <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#validation" target="_blank" rel="noopener noreferrer">constraints</a> and <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#rule" target="_blank" rel="noopener noreferrer">rules</a> which are frequent and complex in the PaaS field through KCL native functions, that is, writing stable and scalable business models, business logic, error-proofing constraints, and environmental rules. The Konfig repository becomes a unified programming interface, workspace and business layer, while the KCL-oriented writing paradigm, which is secure and consistent, with low noise, low side effect and easy to automate, are more beneficial for long-term management and governance.</p><p><img loading="lazy" src="/assets/images/kcl-dev-f53b8dffa98db3aeba220f1403c6eaed.png" width="1500" height="910"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-divide-and-conquer-deconstructing-the-scaling-problem">3. Divide and Conquer: Deconstructing the Scaling Problem<a class="hash-link" href="#3-divide-and-conquer-deconstructing-the-scaling-problem" title="Direct link to heading">​</a></h2><p>The idea of ​​divide and conquer is the key to solving the scaling problem, whose efficiency is reflected from MapReduce to Kubernetes. In the field of large-scale delivery and operation, the classic operation platform tries to use the built-in unified model, orchestration, and provision technology in a black-box product to deal with full-scale business scenarios. Such a practice can be started easily and quickly and turns out to be effective on a small scale. However, as the adoption rate of different business groups increases and different requirements are put forward, it gradually enters into a state of fatigue with the constantly growing platform technology.</p><p><img loading="lazy" src="/assets/images/classic-plats-0e5580742fe5fb7b9d832a1dc235eb36.png" width="1500" height="901"></p><p>In Ant Group's practice, Konfig monorepo is the programming workspace opened by the internal engineering platform to developers, helping application developers to write configurations and policies around the application operation life cycle with a unified programming interface and tech stack, to integrate with existing and future platform and infrastructure, to create and manage cloud-native environments and RBAC-based permissions on demand, and to manage the delivery workflow through GitOps. Konfig monorepo provides an independent white-box programming space for different scenarios, projects and applications, whose intrinsic scalability comes from:</p><ul><li>Flexible, scalable, independent client-side <a href="https://kusionstack.io/docs/user_docs/concepts/konfig" target="_blank" rel="noopener noreferrer">engineering structure design</a></li><li>The <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#-operators-1" target="_blank" rel="noopener noreferrer">automatic merging</a> technology of isolated config blocks supports the arbitrary and scalable organization of config blocks</li><li><a href="https://kusionstack.io/docs/reference/lang/lang/tour/#type-system" target="_blank" rel="noopener noreferrer">Static type system</a> technology provides reusable and scalable type-based modeling and constraints in a modern programming language manner</li><li>Project-grained GitOps CI workflow definition support</li><li>Provision technology selection based on <a href="https://github.com/KusionStack/kusion" target="_blank" rel="noopener noreferrer">Kusion</a> engine</li></ul><p>Konfig monorepo provides divide-and-conquer, composable engineering structure design, code organization, modeling, workflow and provision tech selection support, meanwhile, it carries scalable business requirements with a consistent development model and workflow. The working method of the client-side ensures flexibility, scalability, and portability while reducing the steadily increasing pressure on the extension mechanisms of the server-side, such as Kubernetes API Machinery.</p><p>The following figure illustrates a typical automated workflow with a GitOps approach in the Konfig monorepo, starting from project-oriented code changes and reaching the runtime through configurable CI and CD processes. Compared to centralized black-box products, this mechanism is much more open, customizable and extensible, through which there’s no more necessity of designing clumsy configuration management portals for different business scenarios, projects, and applications.</p><p><img loading="lazy" alt="image.png" src="/assets/images/d-c-overview-0d3ba9e610b9a7497e5044e7de60124d.png" width="1500" height="985"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-modeling-marginal-revenue-and-the-long-tail">4. Modeling: Marginal Revenue and the Long Tail<a class="hash-link" href="#4-modeling-marginal-revenue-and-the-long-tail" title="Direct link to heading">​</a></h2><p>With the divide-and-conquer white-box engineering structure design, code organization, modeling, workflow and provision tech selection, we also need to consider how to work with platform APIs. The typical dispute within the enterprise is whether to face directly the platform details or to design an abstraction, which eventually grows into such kind of dispute between explicit and implicit concepts.</p><p>The abstract implicit method is a usual choice for platform engineers for non-expert end users, who want to design an easy-to-understand and easy-to-use application model or Spec abstraction, which is isolated from specific platform technical details, in order to reduce the end-user's burden on cognizing and enhances the performance of preventing errors by reducing detail perception. However, most operation platform developers tend to design a powerful and unified application model or Spec abstraction. In practice, they often encounter the following obstacles:</p><ul><li>With the increasing adoption rate of different business groups within the enterprise, it is difficult to implement unified modeling. The most typical case in Ant Group is the huge difference between Infra projects and SaaS applications. Java-based SaaS applications are easy to be unified, while Infra applications often need to be designed separately.</li><li>With a large number of platform technologies in the enterprise, the unified model itself is difficult to be stabilized, especially in response to continuously changing business needs and platform demand growth driven by technology. In Ant Group's practice, delivery and operation are affected by various factors and thus have great instability. At the same time, the business requirements for <code>deliverable</code>, <code>runtime</code>, <code>security</code>, and <code>instrumentation</code> around the application operation life cycle are also increasing. Taking instrumentation as an example, the rapidly increasing demand for application runtime observability and SLO definition in the past two years directly drives changes in end-user usage.</li><li>The general problem of abstract models is that a reasonable model needs to be well designed for end users, meanwhile, the API details of the platform must be kept in sync.</li></ul><p>In Ant Group's practice, we adopt an abstract model for end-users, which refers to application developers, and solve several key problems through the following ideas:</p><ul><li>Modeling for typical applications or scenarios (such as Ant Group's Sofa application), these models are developed by platform developers together with platform SREs and maintained together with application developers to achieve a balance between user experience, cost and standard compatibility. In Ant Group's practice, the information entropy convergence ratio of the abstract model is about 1:5, and the marginal benefit of modeling investment is guaranteed through extensive high-frequency usage.</li><li>For non-typical user applications or scenarios, platform developers and platform SRE support application developers to design application models. Mechanisms such as KCL <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#schema" target="_blank" rel="noopener noreferrer">schema</a> and <a href="https://kusionstack.io/docs/reference/lang/lang/tour#protocol--mixin" target="_blank" rel="noopener noreferrer">mixin</a> help users to model, abstract, inherit, combine, reuse, and reduce repetitive code. Such modeling design work is one of the key points in the field of application PaaS, and we need a more reasonable division of labor for such a scenario. Finally, a large number of "non-standard" platform applications were adopted and managed in an accordant way within Ant Group for the first time, which solved the long tail problem effectively. In a typical collaborative mode, platform developers and platform SREs write base components of platform capability, thus becoming “Enablers”, and helping application developers "build building blocks” quickly by using base components to complete their application models.</li><li>For platform technology, we provide the <a href="https://kusionstack.io/docs/reference/cli/openapi/" target="_blank" rel="noopener noreferrer">generation tool</a> from platform API Spec to KCL schema code, natively support the compile-time selection of different Kubernetes API versions through <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#multi-file-compilation" target="_blank" rel="noopener noreferrer">combined compilation</a>, and solve the flexible requirements of mapping application models to different versions of Kubernetes clusters in internal practice. At the same time, KCL supports the writing of in-schema <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#validation" target="_blank" rel="noopener noreferrer">constraints</a> and independent environmental <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#rule" target="_blank" rel="noopener noreferrer">rules</a>. In addition, KCL also provides the <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#decorators" target="_blank" rel="noopener noreferrer">deprecated decorator</a> to support the force deprecation of the model or model attribute. Through robust complete modeling and constraint mechanism on the client side, general problems such as configuration errors and schema drift are exposed at compile time. Due to the left-shifted problem found before runtime, runtime errors or failures are avoided while pushing to the cluster, which is also a necessary requirement for the stability of the production environment in the enterprise, especially in an enterprise with high-level risks.</li></ul><p>Expert users of the underlying platform technology are usually very familiar with a specific technical domain and prefer to work in an explicit way that faces platform details. The KCL language provides the necessary dynamic and modular support and ensures stability through a static type system and constraint mechanisms. However, the explicit method cannot solve the problem that expert users are not familiar with the details of using cross-domain platform technologies, nor can it solve the problem of the scalability and complexity of platform technologies today. In Ant Group's small-scale YAML-based explicit engineering practice, facing a large number of highly open and configurable platform technologies, the complexity grows continuously with the utilization rate of platform technologies, and ends up in a rigid state that is hard to read, write, constrain, test, and maintain.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-automation-new-challenges">5. Automation: New Challenges<a class="hash-link" href="#5-automation-new-challenges" title="Direct link to heading">​</a></h2><p>Automation is a classic domain in the field of infrastructure operation. With the wide and rapid adoption of cloud-native concepts and technologies, the ability to automatically integrate turns out to be the basic requirement of enterprise operation practice. Open source, highly configurable CI and CD technologies are gradually adopted by enterprises. The black-box "product" approach that cannot be integrated is gradually weakened and replaced by a flexible orchestration approach. The main advantages of this practice lie in its powerful customize orchestration and linking capabilities, high scalability as well as good portability. Especially in the Kubernetes ecosystem, the GitOps method has a high adoption rate and a natural affinity with configurable CI and CD technologies. Such changes are also promoting the gradual transformation of the work order and product centric workflow into a self-service and engineering efficiency platform centric workflow, and the operational capability of the production environment has become an important part of the automatic workflow. In the open source community, the technology innovation of the abstraction layer for different engineering efficiency platforms is also active in progress. The developers on the platform side hope to get through the CI and CD process applied to the cloud environment through the shortest cognitive and practical path.</p><p>In Ant Group's engineering practice, the engineering efficiency platform is deeply involved in the open automation practice of Konfig monorepo, and our practice direction is also highly aligned with the technological roadmap of the engineering efficiency platform. In the collaborative work of several people to dozens or even hundreds of people, workflow design for operation scenarios, high-frequency code submission and pipeline execution, real-time automated testing and deployment pose several challenges to the engineering efficiency platform. Especially the diverse businesses in monorepo require independent and powerful workflow customization and operation support, as well as parallel workflow execution capabilities with high real-time and strong SLO guarantee. The requirements of the single-repository mode are hugely different. Most configuration languages are interpreted languages, while KCL is designed as a compiled language, implemented by Rust, C and LLVM optimizer, to provide <a href="https://kcl-lang.io/blog/2022-declarative-config-overview#35-performance" target="_blank" rel="noopener noreferrer">high-performance</a> compile-time and runtime execution for large-scale KCL files. At the same time, KCL can be compiled to native code and wasm binary to meet various runtimes execution requirements. In addition, the storage and architecture design of Git is different from the <a href="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext" target="_blank" rel="noopener noreferrer">Citc/Piper</a> architecture, and not suitable for monorepo of large-scale code. Fortunately, we have not encountered a big problem with the scale of code today. Meanwhile, we’re working together to solve problems and hope to solve them gradually as the practice deepens.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-collaborative-culture-a-more-important-thing">6. Collaborative Culture: A More Important Thing<a class="hash-link" href="#6-collaborative-culture-a-more-important-thing" title="Direct link to heading">​</a></h2><p>The above technologies, tools and mechanisms are very important, but I must say that for engineering and DevOps, the culture of collaboration, cooperation and sharing among groups and teams is more important, because it’s work composed of people, which means people and culture are actually the key points. Within an enterprise full of team silos or isolated teams, people tend to pursue a closed and bad engineering culture, thus we usually see a large number of private code bases and private documents, the judging and working method of small isolated groups. Teams that are supposed to work closely together actually do their own thing and chase after their own private goals. I think it’ll be very difficult to do anything at scale in a culture like this. So if your company or team plans to adopt DevOps at scale, the most important thing is to communicate extensively and to start the culture construction, because it’s not just such a kind of issue concerning a few persons, and it’s also quite difficult and unmanageable.</p><p>In Ant Group's practice, there are always various difficulties in the initial stage, and everyone's concerns about the self-service mechanism and collaborative culture are particularly obvious, such as "I need to write code?" "My code is actually in the same codebase with other teams?", "The job I'm in charge of is not easy, it won't work this way" are typical. Fortunately, we ended up with a virtual organization for common goals, with full support from partners and leaders, and we reached a consensus in the mass. Although the application developers are always complaining about the technology, workflow and mechanism, and hope to obtain a better user experience, which is understandable in deed, the application developers themselves are not the meant obstacle. The real obstacle firstly comes from the operation platform dev team itself. I see that the DevOps ideal of some companies turns into that the operation platform team does all the work for the application developers, even prevents users from accessing labor tools such as code and toolchains, and rushes to hide behind the existing GUI product, which deviates from the original intention and underestimates the capabilities and creativity of the users. Secondly, the obstacles also come from the technical leaders of some platform technical teams. It is difficult for them to give up focusing on the existing work that has lasted for many years, and then accept a new user service model. The feasible way is to let them understand the meaning and vision of the work, and gradually exert influence on them step by step and stage by stage.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="7-summary">7. Summary<a class="hash-link" href="#7-summary" title="Direct link to heading">​</a></h2><p>After more than a year of practice, 400+ developers have directly participated in the code contribution of Konfig monorepo, and managed more than 1500 projects. Among them, the ratio of platform developers together with platform SREs to application developers is less than 1:9. Some of these application developers are the application owners themselves, and some are representatives of the application dev team, which is up to the application team itself. Through continuous automation capability building, 200-300 commits occur every day based on the Konfig monorepo, most of which are automated code modifications, about 1k pipeline task executions and nearly 10k KCL compilation executions. Today, if you compile the full code in Konfig once and output it, 300W+ lines of YAML text will be generated. A release operation process requires multiple compilations with different parameter combinations. Through the lightweight and portable code base and toolchains, we have completed a significant external private cloud delivery, eliminating the pain of transforming and porting a series of legacy operation platforms. Inside Ant Group, we have served several different operation scenarios, and are expanding the scale and exploring more possibilities.</p><p>Finally, I would like to talk about the next step. There’re still possibilities and potentialities for us to keep improving our technology and tools in terms of usability and user experience. More user feedback and continuous improvement are always needed, and there is no shortcut to a good user experience. In the aspect of testing, we provide simple integration testing methods, which play the role of smoke testing, however, it is far from enough. We are trying to ensure correctness based on schema constraints and environmental rules rather than tests. In terms of work interface, we hope to build an IDE-based dev workspace and continue to enhance and optimize the internal workflow experience. At the same time, we hope to continue to improve coverage and technical capabilities. In addition, we also hope to apply the practice method more widely in CI and operation workflow, as well as other scenarios, to shorten end-user awareness and end-to-end workflow. At present, KusionStack is still in the very early stage of open source, and there is a lot of work to do in the future. The most important thing is that we hope to share the concepts and practices of our platform engineering with more companies and teams, to bring about and witness some interesting changes together.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="8-reference">8. Reference<a class="hash-link" href="#8-reference" title="Direct link to heading">​</a></h2><ul><li><a href="https://kusionstack.io/docs/user_docs/intro/kusion-intro" target="_blank" rel="noopener noreferrer">https://kusionstack.io/docs/user_docs/intro/kusion-intro</a></li><li><a href="https://platformengineering.org/blog/what-is-platform-engineering" target="_blank" rel="noopener noreferrer">https://platformengineering.org/blog/what-is-platform-engineering</a></li><li><a href="https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/" target="_blank" rel="noopener noreferrer">https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/</a></li><li><a href="https://web.devopstopologies.com/#anti-types" target="_blank" rel="noopener noreferrer">https://web.devopstopologies.com/#anti-types</a></li><li><a href="https://github.com/KusionStack/kusion" target="_blank" rel="noopener noreferrer">https://github.com/KusionStack/kusion</a></li><li><a href="https://github.com/KusionStack/KCLVM" target="_blank" rel="noopener noreferrer">https://github.com/KusionStack/KCLVM</a></li><li><a href="https://kusionstack.io/docs/reference/lang/lang/tour/#%E9%85%8D%E7%BD%AE%E6%93%8D%E4%BD%9C" target="_blank" rel="noopener noreferrer">https://kusionstack.io/docs/reference/lang/lang/tour</a></li><li><a href="https://kusionstack.io/docs/user_docs/concepts/konfig" target="_blank" rel="noopener noreferrer">https://kusionstack.io/docs/user_docs/concepts/konfig</a></li><li><a href="https://kcl-lang.io/blog/2022-declarative-config-overview#35-performance" target="_blank" rel="noopener noreferrer">https://kcl-lang.io/blog/2022-declarative-config-overview#35-performance</a></li><li><a href="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext" target="_blank" rel="noopener noreferrer">https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext</a></li></ul>]]></content>
        <author>
            <name>Xiaodong, Duo</name>
        </author>
        <category label="KusionStack" term="KusionStack"/>
        <category label="Kusion" term="Kusion"/>
        <category label="Large-scale" term="Large-scale"/>
        <category label="Platform Engineering" term="Platform Engineering"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sense of KusionStack Open Day]]></title>
        <id>2022-sense-of-open-day</id>
        <link href="https://kusionstack.io/blog/2022-sense-of-open-day"/>
        <updated>2022-06-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[历时两年，打破“隔行如隔山”困境]]></summary>
        <content type="html"><![CDATA[<p><strong>历时两年，打破“隔行如隔山”困境</strong></p><p>本文撰写于 KusionStack 开源前夕，作者有感而发，回顾了团队从 Kusion 项目开发之初到现今成功走上开源之路的艰辛历程。当中既描述了作者及其团队做 Kusion 项目的初心和项目发展至今的成果，也表达了作者自身对团队的由衷感激，字里行间都散发着真情实感。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-kusionstack-是什么">1. KusionStack 是什么？<a class="hash-link" href="#1-kusionstack-是什么" title="Direct link to heading">​</a></h2><p>KusionStack 是开源的可编程云原生协议栈！</p><p>Kusion 一词来源于 fusion（意为融合），希望通过一站式的技术栈融合运维体系的多个角色，提升运维基础设施的开放性、扩展性，从整体上降本增效。KusionStack 通过定义云原生可编程接入层，提供包括配置语言 KCL、模型界面、自动化工具、最佳实践在内的一整套解决方案，连通云原生基础设施与业务应用，连接定义和使用基础设施的各个团队，串联应用生命周期的研发、测试、集成、发布各个阶段，服务于云原生自动化系统建设，加速云原生落地。</p><p><img loading="lazy" src="/assets/images/1-e6d5f19b79120ca8410c3f8c902c3e29.jpg" width="1080" height="720"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-为了一个理想的运维体系">2. 为了一个理想的运维体系<a class="hash-link" href="#2-为了一个理想的运维体系" title="Direct link to heading">​</a></h2><p>2019 年秋，MOSN 的工作已持续了近两年，期间我们逐步完成了在支付宝核心链路的形态验证。整个过程中除了 MOSN 本身面对的种种技术挑战和困难，所谓的云原生技术红利，实际上也已经掣肘于运维系统固化所造成的效率制约。</p><p>有一天主管找我吃饭（下套），期间向我描述了他理想中的运维体系：</p><p>他希望 SRE 能通过一种专用语言来编写需求，通过写代码来定义基础设施的状态，而不是花费极大的精力在检查、发现、修复的循环上。基础设施团队则通过提供开放的可编程语言和工具支撑不同诉求的 SRE 团队，达到更高的整体 ROI。</p><p>我立刻意识到这和 Hashicorp 的 Terraform 神之相似（后来 Hashicorp 在 2021 年底上市，以超过 150 亿美元的市值成为迄今为止市值最高的一次开源 IPO）。另一方面，不同于 IaaS 交付场景，蚂蚁面对着大量更规模化、复杂度更高的云原生 PaaS 场景，又让我想到了 Google 内部运用专用语言、工具等技术开放 <a href="https://pdos.csail.mit.edu/6.824/papers/borg.pdf" target="_blank" rel="noopener noreferrer">Borg</a> 和相关的 <a href="https://sre.google/workbook/configuration-specifics" target="_blank" rel="noopener noreferrer">运维能力的实践</a>，当时感觉这是 <a href="https://queue.acm.org/detail.cfm?id=2898444" target="_blank" rel="noopener noreferrer">一个既有意思又有挑战的事</a>。</p><p>饭桌上我们聊了一些思路以及一些还不太确定的挑战，他问我想不想搞一个试试，搞不成也没关系。当时没想太多，饭没吃完就答应了。</p><p><img loading="lazy" src="/assets/images/2-a982a0478af00482863c7214c62d721a.jpg" width="1080" height="720"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-漫长的学习探索与实践">3. 漫长的学习、探索与实践<a class="hash-link" href="#3-漫长的学习探索与实践" title="Direct link to heading">​</a></h2><p>隔行如隔山。</p><p>没有过语言设计研发的经验，也没有过开放自动化系统设计的经验，项目开展之初，我们就陷入了举步维艰的困境。</p><p>经历了一段漫长时间的学习、摸索和实践的反复循环之后，项目依旧没有大的起色，更困难的是我们不但要面对蚂蚁内部复杂又耦合的场景和问题，还要经受「这种高度工程化的方式在蚂蚁是否有生存土壤」的质疑。</p><p>屋漏偏逢连夜雨，期间又令人惋惜且无奈的经历了一些人事变化，同时由于种种原因，项目一度陷入了各种困境。整个 2020 年，我们在未知、纠结、无奈中度过…… </p><p>感谢瓴熙、庭坚和我的主管，感谢你们当时没有放弃这个项目，依然与我一同坚守。</p><p><img loading="lazy" src="/assets/images/3-1830e015242dc6cdcc1335e65ce85ca6.jpg" width="1080" height="720"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-痛并快乐的孵化之旅">4. 痛并快乐的孵化之旅<a class="hash-link" href="#4-痛并快乐的孵化之旅" title="Direct link to heading">​</a></h2><p>通过持续地布道、交流和沟通，我们逐步在基础设施技术团队和 SRE 团队找到了更多有共识的朋友。</p><p>同时在技术上，我们亦脱离了迷茫，真正意义上地启动了 Kusion 项目，也成功地从 PoC 过渡到了 MVP 的阶段。</p><p>最终，我们以“非标”应用为切入点，开始了痛并快乐着的孵化之旅。</p><p>感谢零执、青河、子波、李丰、毋涯、向野、达远……在这里无法一一列举，感谢你们的坚持让这个想法逐步成为现实。</p><p><img loading="lazy" src="/assets/images/4-f7895e32efe0875e8fab8e6711a5a6dd.jpg" width="1243" height="829"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-突破与进展">5. 突破与进展<a class="hash-link" href="#5-突破与进展" title="Direct link to heading">​</a></h2><p>略过中间的种种探索和实践，回顾这段历程，在这一年多的时间里我们结合了编译技术、运维及平台技术，成功建立了一个基于 Kusion 可编程技术栈的运维体系。</p><p>在业务场景上，项目覆盖了从 IaaS 到 SaaS 的大量运维场景，截至目前共接入了 800+ 应用，覆盖 9 个 BG，21 个 BU，其中典型案例交付运维提效 90% 以上，这也是蚂蚁内部第一次将大量异构应用纳入到一整套运维技术栈。</p><p>在蚂蚁我们基于云原生容器和微服务技术深入探索了 DevOps、CICD 实践，完善了蚂蚁的云原生技术体系，逐步释放了云原生效率红利，同时形成了一个近 300 人的虚拟运维研发团队。</p><p>不同职能不同团队的参与者凝聚在一起解决各自所面对的问题，贡献了 30K+ commit 和 350K+ 行代码，有一些参与者自发成为 Kusion 的研发者 。我认为这些工程师文化理念和领域知识的积累带来了远超运维业务本身的价值。</p><p><img loading="lazy" src="/assets/images/5-3bff5c659fcb099cfbc618a7fdfe781f.png" width="1080" height="794"></p><p>此外，Kusion 也成为了可编程基线产品、云原生运维产品、多云交付产品等新一代运维产品的基础技术，成为蚂蚁运维体系架构升级的一部分。</p><p>不忘初心，我们希望通过技术手段促进与运维参与方的合作关系的合理化、基于开放技术栈的自动化，以及运维数据与知识的沉淀积累，以达到整体协作运维效率的不断提升。</p><p>同时，因蚂蚁内部运维场景较多且链路复杂，每个环节都需要最懂运维业务的 SRE 密切参与，与平台、应用研发协同工作，最终各环节联合在一起形成了一套完整的运维体系，在这样的思路下开放技术也会越来越重要。</p><p>平台研发、SRE、应用研发等多种角色协同编写的代码是一种数据的沉淀，亦是一种业务知识的沉淀，基于这些数据和知识，未来会有更多的可能性。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-走上开源之路">6. 走上开源之路<a class="hash-link" href="#6-走上开源之路" title="Direct link to heading">​</a></h2><p>在历经了一段内部探索之后，我们希望把 KusionStack 开源到技术社区。因为我们意识到自身面对的问题，其他公司、团队其实也同样正在面对。借助开源这件事，我们希望团队的这些工作成果能对更多人有所帮助。</p><p>当然，也受限于自身能力以及精力和资源的投入，我们希望能有更多朋友参与进来，与我们共同去完善 KusionStack，不论你是工作在云原生、运维自动化、编程语言或者是编译器中的哪一个领域，我们都非常期待和欢迎你的加入。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="7-期待与你共成长">7. 期待与你共成长<a class="hash-link" href="#7-期待与你共成长" title="Direct link to heading">​</a></h2><p>这段经历对我来说异常宝贵，不仅仅是在于自身再一次在新的技术领域和蚂蚁的技术升级方面尝试了新的探索并实现了突破，更宝贵的是，自己还拥有了一段与一群人均 95 后的小伙伴一起将想法落地实现的奇幻历程。</p><p>在未来，Kusion 的朋友圈不再局限于蚂蚁内部，面向开源，我们期待着能有更多的社区朋友在 KusionStack 与我们共同成长！</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="参考链接">参考链接<a class="hash-link" href="#参考链接" title="Direct link to heading">​</a></h2><ul><li><a href="https://pdos.csail.mit.edu/6.824/papers/borg.pdf" target="_blank" rel="noopener noreferrer">Large-scale cluster management at Google with Borg (PDF)</a></li><li><a href="https://queue.acm.org/detail.cfm?id=2898444" target="_blank" rel="noopener noreferrer">Borg, Omega, and Kubernetes (PDF)</a></li><li><a href="https://sre.google/workbook/configuration-specifics" target="_blank" rel="noopener noreferrer">Configuration Specifics</a></li></ul>]]></content>
        <author>
            <name>朵晓东</name>
        </author>
        <category label="KusionStack" term="KusionStack"/>
        <category label="Kusion" term="Kusion"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[KusionStack Open Day]]></title>
        <id>2022-open-day</id>
        <link href="https://kusionstack.io/blog/2022-open-day"/>
        <updated>2022-05-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[2022 年 5 月 28 日，KusionStack Open Day 线上线下视频直播正式宣布 KusionStack 一站式可编程配置技术栈（包含 KCL 配置语言、Kusion 引擎、Konfig 配置大库）开源。]]></summary>
        <content type="html"><![CDATA[<p>2022 年 5 月 28 日，KusionStack Open Day 线上线下视频直播正式宣布 KusionStack 一站式可编程配置技术栈（包含 KCL 配置语言、Kusion 引擎、Konfig 配置大库）开源。</p><p><img loading="lazy" src="/assets/images/01-4e482cb7a92685aa88e9eb4c70e59bae.jpg" width="1080" height="575"> </p><p>以上是 KusionStack 杭州团队合影。至此，🎉KusionStack Open Day 圆满结束🎉</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-精彩瞬间">1. 精彩瞬间<a class="hash-link" href="#1-精彩瞬间" title="Direct link to heading">​</a></h2><p>感谢大家抽空参加，我们一起来回顾下👀，本次活动的精彩瞬间吧～👏</p><p><img loading="lazy" src="/assets/images/02-52f93b2bc1eddb898b049dc344c1399d.jpg" width="1080" height="720"></p><p>蚂蚁集团高级技术专家、Kusion 项目发起人及负责人——朵晓东作为整场活动的主持人，带领大家一起回顾了 Kusion 项目演进历程，并宣布了 KusionStack 正式开源的好消息！</p><p><img loading="lazy" src="/assets/images/03-2c9990688b8d85f8ab41a54868c1fe77.jpg" width="1080" height="1011"></p><p>活动开场，Kusion 项目的两位 Sponsor——蚂蚁集团可信原生技术部负责人何征宇和技术风险部负责人陈亮，对 Kusion 从研发至今的发展回顾和未来展望。两位 Sponsor 在视频中给予了 Kusion 项目极大的认可，并为 KusionStack 的开源献上了祝福。</p><p><img loading="lazy" src="/assets/images/04-22bbdda26c3d2efb03eea67d8da5f105.jpg" width="653" height="433"></p><p>Kata 创始人、木兰社区 TOC —— 王旭</p><p>王旭在会上也表达了对 KusionStack 开源的喜悦。他指出：一个项目拿出来开源的好的时机，应当是项目还处在未完全成熟的阶段，这样在开源出来之后，才能够通过开源社区的开发者们，一起推动项目更好的发展，开源不是为了秀肌肉。</p><p>在最后，他再次为 KusionStack 的开源送上了诚挚的祝福。</p><p>接下来是各位嘉宾的精彩分享，一起来回顾一下吧～</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-数字化出海业务的-devops-探索和实践">2. 《数字化出海业务的 DevOps 探索和实践》<a class="hash-link" href="#2-数字化出海业务的-devops-探索和实践" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/assets/images/05-efd253a99042daa806a533e592d54206.png" width="1031" height="644"></p><p>开场演讲的众安国际科技 Engineering 负责人李晓蕾（Sherry Lee），介绍了众安国际 DevOps 在支持数字化出海业务过程遇到难点和对应的解决之道。</p><p>她从数字化出海对 DevOps 带来的挑战、众安国际 DevOps 遇到的难点和解决方案以及 DevOps 具体实践案例分享三个方面展开了宝贵的经验分享。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/0-Sherry-Lee-%E6%95%B0%E5%AD%97%E5%8C%96%E5%87%BA%E6%B5%B7%E4%B8%9A%E5%8A%A1%E7%9A%84DevOps%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%AE%9E%E8%B7%B5.pdf" target="_blank" rel="noopener noreferrer">数字化出海业务的DevOps探索和实践</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1hr4y1x72a" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1hr4y1x72a</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=tYDw__lBcYM" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=tYDw__lBcYM</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-蚂蚁集团规模化-devops-的代际演进探索">3. 《蚂蚁集团规模化 DevOps 的代际演进探索》<a class="hash-link" href="#3-蚂蚁集团规模化-devops-的代际演进探索" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/assets/images/06-d00776afb137ffb78200c1f2a9e110b1.jpg" width="1080" height="720"></p><p>蚂蚁集团高级技术专家、Kusion 项目发起人及负责人——朵晓东分享了 Kusion 的项目背景和发展进程，同时他宣布 Kusion 正式开源，并分享了开源计划。</p><p>目前，基于 Kusion 的新一代 PaaS 体系已逐步应用在蚂蚁众多内外部场景，在多种运维场景覆盖、规模化协同效率提升、多主体/站点交付运维、技术创新运维效率提升等多方面体系出显著的优势和价值。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/1-%E6%9C%B5%E6%99%93%E4%B8%9C-%E8%9A%82%E8%9A%81%E9%9B%86%E5%9B%A2%E8%A7%84%E6%A8%A1%E5%8C%96DevOps%E4%BB%A3%E9%99%85%E6%BC%94%E8%BF%9B%E6%8E%A2%E7%B4%A2.pdf" target="_blank" rel="noopener noreferrer">蚂蚁集团规模化 DevOps 代际演进探索</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1WZ4y147pC" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1WZ4y147pC</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=T6NKkb1L1eM" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=T6NKkb1L1eM</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-kcl-配置策略语言">4. 《KCL 配置策略语言》<a class="hash-link" href="#4-kcl-配置策略语言" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/assets/images/07-f9f31715a2bb4bb27220743a833b187e.jpg" width="922" height="546"></p><p>蚂蚁集团高级研发工程师徐鹏飞介绍了 KCL 的相关核心特性，分享了 KCL 技术栈的思路、架构、关键技术，并展开讲述了 KCL 的在蚂蚁内部多场景的实践经历。</p><p>KCL 帮助不同角色的用户以简单、可扩展、稳定、高效、分而治之的方式完成开发和运维任务，同时支持与自动化系统集成，实现极致的执行效率。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/2-%E5%BE%90%E9%B9%8F%E9%A3%9E-KCL%E9%85%8D%E7%BD%AE%E7%AD%96%E7%95%A5%E8%AF%AD%E8%A8%80.pdf" target="_blank" rel="noopener noreferrer">KCL配置策略语言</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1bv4y1w7ke" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1bv4y1w7ke</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=mUFFri_eRAQ" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=mUFFri_eRAQ</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-中场休息">5. 中场休息<a class="hash-link" href="#5-中场休息" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/assets/images/08-fe322d918ee01bb65bb1e59eec1d7b70.jpg" width="1080" height="720"> </p><p>中场休息期间，“开源老兵”、Go 语言大佬——柴树杉老师浅谈了他参与 KusionStack 的心路历程和个人收获，并表达了能有更多伙伴参与到 KusionStack 的开源共建的希望。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-kusion-模型库和工具链的实践探索和总结">6. 《Kusion 模型库和工具链的实践探索和总结》<a class="hash-link" href="#6-kusion-模型库和工具链的实践探索和总结" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/assets/images/09-33ea5d03f2565c2cfaa7c5b16b7b1055.jpg" width="1828" height="1032"> </p><p>蚂蚁集团高级研发工程师杨英明以实际的案例介绍了如何通过 KCL 抽象 Kusion 模型库，以及如何结合 Kusion 工具链一站式的完成配置代码的编写和生效，同时总结分享了通过这套模式进行实际交付的经验和建议。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/3-%E6%9D%A8%E8%8B%B1%E6%98%8E-Kusion%E6%A8%A1%E5%9E%8B%E5%BA%93%E5%92%8C%E5%B7%A5%E5%85%B7%E9%93%BE%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%8E%A2%E7%B4%A2%E5%92%8C%E6%80%BB%E7%BB%93.pdf" target="_blank" rel="noopener noreferrer">Kusion模型库和工具链的实践探索和总结</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1Vr4y1x7Ty" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1Vr4y1x7Ty</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=HDUm_KrunLY" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=HDUm_KrunLY</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="7-kusion-在蚂蚁的规模化实践">7. 《Kusion 在蚂蚁的规模化实践》<a class="hash-link" href="#7-kusion-在蚂蚁的规模化实践" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/assets/images/10-673f24b95d86ef0d9c9f5ed923999ed0.jpg" width="1582" height="896"> </p><p>最后，蚂蚁集团技术专家史贵明和蚂蚁集团高级运维工程师李治玮共同带来了 Kusion 在蚂蚁的规模化实践的分享。史贵明主要从 PaaS 配置管理的系统架构角度讲述了蚂蚁目前的多云配置管理能力，李治玮则从 SRE 视角下，分享了使用 KCL 解决多种复杂基础设施的交付效率问题和价值。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/4-%E8%8E%AB%E5%9F%8E-%E5%8D%8A%E5%BA%AD-Kusion%E5%9C%A8%E8%9A%82%E8%9A%81%E7%9A%84%E8%A7%84%E6%A8%A1%E5%8C%96%E6%8E%A2%E7%B4%A2%E5%AE%9E%E8%B7%B5.pdf" target="_blank" rel="noopener noreferrer">Kusion在蚂蚁的规模化探索实践</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1xB4y1X7sv" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1xB4y1X7sv</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=F9lZEU5GNYE" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=F9lZEU5GNYE</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="8-未来展望">8. 未来展望<a class="hash-link" href="#8-未来展望" title="Direct link to heading">​</a></h2><p>开源并不代表 KusionStack 已经完成，相反我们还有很多需要完善和改进的地方，同时开源社区和文化也对文档和代码提出了更高的要求。这只是一个开始，希望更多从事相关领域的同学能够参与共建，为国内的云原生、DSL 等新兴领域贡献力量。</p><p>最后，感谢大家的参与🙏</p>]]></content>
        <author>
            <name>Kusion</name>
        </author>
        <category label="KusionStack" term="KusionStack"/>
        <category label="Kusion" term="Kusion"/>
        <category label="KCL" term="KCL"/>
        <category label="KCLVM" term="KCLVM"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[KCL云原生配置策略语言]]></title>
        <id>2021-kcl-intro</id>
        <link href="https://kusionstack.io/blog/2021-kcl-intro"/>
        <updated>2021-08-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[介绍KCL云原生配置语言在蚂蚁的诞生背景、语言特性、实践探索和未来的发展思考。]]></summary>
        <content type="html"><![CDATA[<p>介绍KCL云原生配置语言在蚂蚁的诞生背景、语言特性、实践探索和未来的发展思考。</p><ul><li>简介：<a href="https://giac.msup.com.cn/course?id=15307" target="_blank" rel="noopener noreferrer">https://giac.msup.com.cn/course?id=15307</a></li><li>内容：<a href="https://segmentfault.com/a/1190000040455559" target="_blank" rel="noopener noreferrer">https://segmentfault.com/a/1190000040455559</a></li><li><a href="https://gw.alipayobjects.com/os/bmw-prod/2cb0c283-5f24-485e-b635-b6efac887eba.pdf" target="_blank" rel="noopener noreferrer">PDF下载</a></li></ul><p><a href="https://gw.alipayobjects.com/os/bmw-prod/2cb0c283-5f24-485e-b635-b6efac887eba.pdf" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="KCL云原生配置策略语言" src="/assets/images/talk-cover-7bff56b2738955e27cf16a5ebb1a4506.png" width="1820" height="1366"></a></p>]]></content>
        <author>
            <name>柴树杉</name>
        </author>
        <category label="kcl" term="kcl"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[云原生开放运维体系探索实践]]></title>
        <id>2021-kusion-intro</id>
        <link href="https://kusionstack.io/blog/2021-kusion-intro"/>
        <updated>2021-05-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[本文是云原生开放协同技术探索与实践一阶段的总结和综述。]]></summary>
        <content type="html"><![CDATA[<p><em>本文是云原生开放协同技术探索与实践一阶段的总结和综述。</em></p><p>蚂蚁基础技术在过去3年多以来持续、深入推进全面的云原生化技术演进，我们将在线、离线计算资源装进了一台计算机，将服务体系通过 mesh 的思路和技术手段进行了下沉解耦，可以说比较充分的拥抱了云原生技术，并获取了其带来的技术红利。</p><p>当完成了资源、服务的云原生化，我们发现在云原生基础能力之上的运维体系与云原生技术开放、共享的思路有较大的距离，在技术体系上也与云原生技术声明式、白盒化的思路相悖，同时由于缺少匹配的技术支撑，历史包袱等问题也成为了云原生运维难以真正代际演进的障碍。今天我要介绍的就是蚂蚁在这样的背景下在云原生运维方向进行的技术探索和实践。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-规模化云原生运维探索">1. 规模化云原生运维探索<a class="hash-link" href="#1-规模化云原生运维探索" title="Direct link to heading">​</a></h2><p>我们先来回顾一下在蚂蚁真实的实践方式和面对的问题。首先，我们来看看蚂蚁践行多年的经典运维中台，这类运维平台一般包括了控制器、业务模型、编排引擎、原子任务及管道，在蚂蚁这样的平台是一系列服务的集合，他们较好的满足了集中式、标准化、低变更频率的应用发布及运维需求。但这种模式在实践中也存在着明显的不足。</p><p>首先对于非标准应用、应用个性化需求、高成本需求、非紧急需求、技改类需求，往往无法较好的满足。在蚂蚁的实践中，非标运维需求、对核心应用模型及运维模型冲击较大的高成本改造需求、大量基础能力或运维功能的透出需求等长期无法得到较好的满足，需求往往是合理的，是难以获得足够的优先级执行落地。在研发阶段，运维平台长期积累了高复杂度的业务逻辑，修改测试涉及跨系统的长改造链路，同时基础能力的透出、运维能力的产品化依赖前端、服务端研发资源。这些问题使得运维平台研发日渐吃力，特别是在产品 GUI、业务模型、编排引擎等变更热点上，受限于扩展机制能力不足，内部实践中甚至出现过线上不断修改代码、发布服务以满足需求的情况。平台上线后，统一的质保和线上全链路功能验证同样面对较大的压力。对于最终的使用者，命令式按钮背后的黑盒计算透明度低，审计难，结果难预测，同时激情操作、操作界面不熟悉等问题也一直影响着线上的稳定性。这些问题长期存在，我们寄希望于代际的技术演进来解决这些问题。</p><p><img loading="lazy" src="/assets/images/01-79bb6cdf65fde251f50d5226ab5c2529.png" width="1758" height="716"></p><p>当云原生基础服务逐渐稳定后，对于自身场景不在运维平台管理范围内的应用，研发同学自发的借助云原生社区工具链解决问题。基于 Kubernetes 生态高度开放、高度可配置的特点，研发者可以自助、灵活、透明的声明式应用运行、运维需求，以应用粒度完成发布、运维操作。</p><p>用户通过 kustomize 等社区技术缩短了对接基础设施的路径，并通过如 velocity 等文本模板技术部分解决了静态 YAML 文件在较多变量时维度爆炸的问题，解决了默认值设定的问题，同时通过 code review 的方式进行多因子变更及评审。由于 Kubernetes 及其生态提供了面向资源、服务、运维、安全的横向能力，使得这种简单的方式可有很好的普遍性和适用性，通过对不同的 Kubernetes 集群 “播放” 这些数据即可完成对基础设施的变更，本质上是一种声明数据的流转。面向 git 仓库的研发方式和 gitops 流程支持对运维产品研发资源的诉求较低，往往可以比较简单的搭建起来，不强依赖产品研发资源投入。相比经典运维中台，这些好处清晰明确，但从工程视角缺点也非常明显。</p><p><img loading="lazy" src="/assets/images/02-22efc6ce991ff8f5069ed14ce217bd95.png" width="1536" height="798"></p><p>首先 Kubernetes API 的设计较为复杂，仅是 Kubernetes 原生提供的 low level API 就暴露了 500 多种模型，2000 多字段，场景上几乎涵盖了基础设施应用层的方方面面，即使是专业同学也很难理解所有细节。其次这种方式的工程化程度很低，违反 DRY 原则，违反各团队职责能力高内聚低耦合的原则，即使在有一定的工具支持的情况下，在内部的典型案例中一个多应用的 infra 项目仍然维护了多达 5 万多行 YAML，同时由于团队边界造成的多个割裂的平台，用户需在多个平台间切换，每个平台的操作方式各异，加上跳板机黑屏命令，完成一次完整的发布需要 2 天时间。</p><p>由于低工程化程度的问题，各团队间协同依赖人肉拉群同步，最终 YAML 由多个团队定义的部分组合而成，其中一大部分属于 Kubernetes 及运维平台团队的定义，这些内容需要持续跟踪同步避免腐化，长期维护成本高。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-kusion-云原生开放协同技术栈">2. KUSION: 云原生开放协同技术栈<a class="hash-link" href="#2-kusion-云原生开放协同技术栈" title="Direct link to heading">​</a></h2><p>以上两种模式各有利弊，优势和问题都比较清晰。那么能不能既要也要呢，能不能在继承经典运维平台优势的情况下，充分利用云原生技术带来的红利，打造一个开放、透明、可协同的运维体系？</p><p>带着这样的问题，我们进行了探索和实践，并创建了基于基础设施代码化思路的云原生可编程技术栈 Kusion。</p><p>大家都知道 Kubernetes 提供了声明式的 low level API，提倡其上生态能力通过 CRD 扩展的方式定义并提供服务，整个生态遵循统一的 API 规范约束，复用 API 技术和工具。Kubernetes API 规范提倡 low level API 对象松耦合、可复用，以支持 high level API 由 low level API “组合” 而成。Kubernetes 自身提供了利于开源传播的极简方案，并不包括 API 之上的技术和方案。</p><p>回到云原生技术的本源，我们回看了 Kubernetes 前身 Borg 的应用技术生态。如下图示，在 BorgMaster 之上，Borg 团队研发了 Borg 接入三件套，即 BCL（Borg Configuration Language），Command-line tools，以及相应的 web service。用户可以通过 BCL 声明式编写需求，通过 Command-line tools 将 BCL 文件执行到 Borg 集群，并通过 web GUI 视图查看任务细节。经过大量的调研，我们了解到 Google 内部的运维能力及产品生态、质量技术生态都依赖这三件套构建而成，在内部也进行了多年的迭代演进。</p><p><img loading="lazy" src="/assets/images/03-5eca61ccc90511012f710d0f986f412d.png" width="290" height="278"></p><p>这给了我们启发，今天我们有了容器技术、服务体系，有了大量用户和差异化的需求，有了一定数量的自动化运维平台，我们希望能通过云原生专用的语言和工具来链接 Kubernetes 生态、各个运维平台以及大量的用户，通过唯一事实定义消除运维平台孤岛，完成云原生基础设施在应用、运维层面的代际演进，达到 “Fusion on Kubernetes” 的目标。</p><p>带着这样的目标，我们持续地进行做技术探索和实践，目前已经形成了 Kusion 技术栈，并在蚂蚁的生产实践中进行应用。</p><p><img loading="lazy" src="/assets/images/04-2fb71395a1c3d40287ee376b83a8d81d.png" width="1306" height="1078"></p><p>Kusion 技术栈基于这样的基础能力而工作，包括如下组成部分：</p><ul><li>云原生配置策略专用语言 KCL (Kusion Configuration Language)</li><li>KCL 解释器及其 Plugin 扩展机制</li><li>KCL 研发工具集: Lint, Format, Doc-Gen，IDE Plugin(IDEA, VsCode)</li><li>Kusion Kubernetes 生态工具: OpenAPI-tool, KusionCtl(Srv)</li><li>Konfig 配置代码库，其中包括平台侧及用户侧代码</li><li>OCMP (Open CloudNative Management Practice) 实践说明书</li></ul><p><img loading="lazy" src="/assets/images/05-d5a6d789ec8b357c181433c90cae65d5.png" width="2362" height="1160"></p><p>Kusion 工作在基础设施之上，作为抽象及管理层的技术支撑服务上层应用。不同角色的用户协同使用 Kubernetes 生态提供的横向能力，通过声明式、意图导向的定义方式使用基础设施，在场景上支持典型的云原生场景，也服务了一些经典运维场景，完成了一阶段的建设工作。目前接入 Kusion 的产品包括 IaC 发布、运维产品 InfraForm、建站产品 SiteBuilder、快恢平台等。通过将 Kusion 集成在自动化系统中，我们尽可能的调和黑盒命令式自动化系统与开放声明式配置系统，使其发挥各自的优势。</p><p><img loading="lazy" src="/assets/images/06-db08a31e60ea42621297d1b6e86b506f.png" width="710" height="542"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-集成--落地">3. 集成 &amp; 落地<a class="hash-link" href="#3-集成--落地" title="Direct link to heading">​</a></h2><p>新的技术体系首先面临着落地的问题，我们先来看看 Kusion 在集成落地方面的思考和做法。</p><p>从整体思路上，我们从经典运维系统中的变更热点业务层、编排层着手，以 KCL 声明式配置块的方式外置编写对应逻辑，并被控制器自动化集成。</p><p>这种思路是有迹可循的，我们来看看同行的经验，以雷神山医院的建设现场为例，我们可以看到现场大量的组件是预制品，经过了测试、验证、交付后由现场的塔吊负责组装。这些组件需要良好的品控，需要内置水管、电线等“能力”，否则即使组装也无法有效工作，同时需要给业务侧一定的自定义配置空间，还要易于组装及自动化以提升现场装配效率。实际上我们面对的大规模运维活动与这样的现场有类似之处，现代基建的高效手段非常值得我们学习借鉴。</p><p><img loading="lazy" src="/assets/images/07-a8b81a2606564bb234a8d54d0d3f58f1.png" width="956" height="666"></p><p>对应我们的实际场景，我们基于 KCL 的工作方式需要满足以下要求：</p><ul><li><strong>可分工、可协同：</strong>组件制作、验收、交付、使用可以基于角色需要合理分工协同，满足软件供应链的需求</li><li><strong>外置、预制的组件：</strong>组件独立于自动化系统存在，预制的组件在交付前需要经过充分的测试验证</li><li><strong>内置资源、服务、身份等要素：</strong>组件仅向用户暴露有效的业务信息，同时内置云原生、可信的逻辑</li><li><strong>易于业务定义：</strong>组件需要提供一定的自定义配置能力</li><li><strong>易于自动化：</strong>支持自动化组装，自动化对组件进行“增删改查”</li></ul><p>接下来，我们来看看基于 Kusion 工作的典型流程，此处有一定的抽象和简化。</p><p>前文提到 Kubernetes API 提供了基于 OpenAPI 的 low level API 及扩展机制，基于高内聚、低耦合、易复用、易组装的原则设计，以 Resource、Custome Resource 的方式存在。此外，Kubernetes API 提供了大量的命令以操作容器、Pod 等资源。对于 SDN、Mesh，或是其他的能力扩展都是基于这样的整体约束和方式，大都提供了资源定义或命令操作。</p><p>基于这样的基础，在蚂蚁的实践中我们将整体的工作流程分为 4 个步骤：</p><ul><li><strong>代码化:</strong> 对于资源定义，基于 OpenAPI Model/CRD 定义生成 KCL 结构体；对于命令操作，编写对应的声明式 KCL 结构体。这些结构体对应到平台侧原子能力定义。</li><li><strong>抽象化:</strong> 平台侧 PaaS 平台同学基于这些原子声明式编写抽象、组装，并定义出面向用户的前端结构体，从功能场景上涵盖了 AppConfiguration, Action / Ops, Locality / Topology, SA / RBAC, Node / Quota 等场景，并提供了简化编写的 Template 集合。以 AppConfiguration 为例，我们提供了SigmaAppConfiguration、SigmaJobConfiguration 分别对应于服务型和任务型应用定义，此外针对 SOFA 应用的特征提供了 SofaAppConfiguration。这些前端结构体作为 Kusion Models 的“接口层”存在，受限于业务进度等原因各场景积累的水位不同，仍需要长期的积累打磨。</li><li><strong>配置化:</strong> 应用侧研发或 SRE 同学基于这些前端结构体描述应用需求。用户可以通过结构体声明的方式为应用定义配置基线及不同环境的配置。在大部分情况下，用户仅需要进行结构体声明，即一些 key-value 对。对于有复杂需求的场景，用户可以进行逻辑编写或通过继承结构体的方式组织代码逻辑</li><li><strong>自动化:</strong> 当应用侧配置完成后，实际上已经定义好了可用的“组件”，具备了自动化的条件。平台侧控制器可以通过 KCL CLI 或 GPL binding API 完成编译、执行、输出、代码修改、元素查询等自动化集成工作，用户则可以通过 KusionCtl 工具执行 KCL 代码映射执行到 Kubernetes 集群。</li></ul><p><img loading="lazy" src="/assets/images/08-44399e28b0f5b79daa744587a0856234.png" width="2356" height="1154"></p><p>通过这样统一的工作流程，我们轻量级地完成了对 Kubernetes 生态大量基础能力的透出，基于原子能力声明式地封装、抽象出面向应用的配置、运维能力，并完成了一定场景的落地应用。Kusion 提供了研发工具协助使用者完成其工作。我们可以对平台侧、用户侧分层协同模式下的实践做进一步的探讨。平台侧同学抽象并定义出前端结构体，例如 SofaAppConfiguration ，其中定义了业务镜像、所需资源、config、secrect、sidecar、LB、DNS、副本数、逻辑资源池、发布策略、是否超卖、是否访问公网等等。</p><p>前端结构体无法独立工作，实际上存在着与前端结构体对应的后端结构体，后端对前端透明，前-后端结构体分离解耦。后端结构体在运行时将前端结构体产生的数据“翻译”成对应的 low level API ，这种反向依赖的方式依赖于 KCL 语言能力。</p><p>从工程角度看平台侧同学实际上完成了一次轻量级、声明式的应用级 API 定义。这种前后端分离的设计有诸多好处。首先应用侧使用的前端结构体可以保持简单干净、业务导向、实现细节无关；其次可以通过编译时指向不同的后端文件动态切换到不同的后端结构体实现，以完成平台版本切换、实现切换等目的；最后这样分离的做法可以在统一模式的前提下保证充分的灵活性，例如平台可以通过 kcl base.k prod.k backend.k 多文件编译完成一次包含基线、环境配置、后端结构体的组合编译。事实上，我们可以将所有场景规约为 kcl user_0.k … user_n.k platform_0.k … platform_n.k 的范式，其中 user.k 代表用户侧代码，platform.k 代表平台侧代码。我们从另一个角度来看多团队协同的方式。由各团队自下而上定义平台能力及约束，并完成应用级的配置基线及配置环境特征，完成最后一公里的定义。</p><p><img loading="lazy" src="/assets/images/09-7a74ef990a90b1319880938d66e8a31c.png" width="2150" height="1126"></p><p>在理清工作流程后，我们来看 KCL 通过 Konfig 大库落地的实践。我们在 Konfig 代码仓库中定义了平台侧及用户侧的代码空间，通过统一配置代码库完成对代码的共享和复用，保证了对整体基础设施代码定义的可见性。在用户侧，通过 project、stack、component(对应蚂蚁内部应用) 三级目录的方式组织代码。以 cloudmesh 为例，在 tnt/middleware/cloudmesh 的 project 目录下含多个 stack，如 dev、prod，每个 stack 中含多个 component。代码在这三个维度得以隔离，并共享上下文。</p><p><img loading="lazy" src="/assets/images/10-459ff8a8b60ed9a731b3c969bec76a2f.png" width="1264" height="852"></p><p>在代码质保方面，我们通过单元测试、集成测试等手段保证对平台侧、用户侧代码的质量，我们正在引入代码扫描、配置回放、配置校验、dry-run 等验证手段保证代码变更的可靠性。在研发方面，我们通过主干开发、分支发布的方式保证不同应用并行研发的前提下尽可能不产生代码腐化的情况，并通过 tag 保护稳定分支。</p><p><img loading="lazy" src="/assets/images/11-9288b20e7e93ea9514ce49323ae1732c.png" width="2008" height="398"></p><p>在 IaC 产品落地场景中，通过标准化的结构体、代码版本化、多环境代码隔离、CI pipeline 等手段管理基础设施描述代码，通过代码变更的静态、动态 diff、模拟、异常提示、风险管控接入保证基础设施变更可控，通过代码 Pull Request 做变更审计及对变更人员的追踪。下图以业务发布场景为例展示了关键步骤，在业务代码通过质保流程并完成镜像构建后，CI 流程控制器通过 KCL API 对 Konfig 仓库中对应 KCL 文件中的 image 字段进行自动更新，并发起 Pull Request，由此触发发布流程。</p><p>IaC 提供了编译测试、live-diff、dry-run、风险管控接入等验证方式，并支持执行过程的可视化，产品基于 KCL 语言能力及工具建设，尽可能的减少业务定制。整个流程以 Konfig 代码的自动修改为起点，平台方、应用方、SRE 基于代码协同，通过产品界面进行线上发布，支持分批分步、回滚等运维能力。Konfig 中的代码“组件”可以被多个场景集成使用，例如此处被发布控制器集成的组件还可以被建站控制器集成，控制器只需关注自动化逻辑，无需关心被集成组件的内部细节。以文章开头的典型建站场景为例，在接入 Kusion 后，用户侧配置代码减少到 5.5%，用户面对的 4 个平台通过接入统一代码库而消减，在无其他异常的情况下交付时间从 2 天下降到 2 小时。</p><p><img loading="lazy" src="/assets/images/12-195ae3b012287a996749e577b427df62.png" width="2218" height="846"></p><p>我们再来看更加动态性的大规模快速恢复场景。快恢平台在接到监控告警输入后决策产生异常容器 hostname 列表，并需要对容器进行重启等恢复操作。</p><p>我们通过 KCL 编写声明式的应用恢复运维代码，其中通过 KCL Plugin 扩展完成对在线 CMDB 的查询，将 hostname 列表转换为多集群 Pod 列表，并声明式定义 Pod 恢复操作。快恢平台执行 KusionCtl run AppRecovery.k 完成跨多集群的 Pod 恢复操作。通过这样的方式，快恢控制器无需理解容器恢复细节、Kubernetes 多集群映射执行细节等，可以更专注于自身异常判断及决策逻辑。</p><p><img loading="lazy" src="/assets/images/13-06bb5d00c402a5b01df6e1b91f4547d3.png" width="2236" height="1154"></p><p>在项目落地过程中，我们也发现到了不少因为进度等原因造成的平台侧设计问题。例如平台侧操作定义不够规范，应用依赖等共性定义过于分散等问题，这需要我们在后续的落地过程中持续去沉淀提高。开放配置给了用户更大的灵活性和空间，但相比黑盒的方式需要更多的安全性保障。在开放协同推进的同时，可信原生技术部在并行推进云原生可信平台的建设，可信平台通过将身份与 Kubernetes 技术紧密结合提供相比社区方案能力更强的技术支撑。</p><p>举个例子，通过开放配置我们是不是可以通过 mount 证书的方式使得不可信不安全的服务获得访问目标服务的权限从而获取到关键数据？事实上在没有身份传递及高水位 Pod 安全保障的前提下这是完全可能。通过可信平台对 PSP（Pod Security Policy）、服务验证、服务鉴权等场景的加固，使得我们可以按需增强关键链路的安全策略。相比与社区方案，可信平台定义了更完整的 spiffe 身份标识，并使得身份作用于资源、网络、服务的各个环节，可以说可信是开放的必要前提。同时可信提供的鉴权能力、隔离能力也需要被用户使用，将原子能力封装并在应用配置层面透出依赖于 Kusion 的推进，使得接入 Kusion 的应用可以更简单的使用可信能力。可以说开放协同技术栈与可信平台是能力正交，相辅相成的云原生应用层技术。</p><p><img loading="lazy" src="/assets/images/14-adbbc347994d20aae4ce0916250c76f1.png" width="960" height="441"></p><p><strong>最后，我们对集成落地做一个小结：</strong></p><p>平台侧编写 80% 内容，通过面向应用的前端结构体提供规范的配置块，再通过后端结构体定义屏蔽 low level API 资源及操作，最终通过这样的方式描述应用对 workload、编排、运维等方面的需求，重点在于可以定义什么、默认有什么及约束集合，并通过 Konfig 仓库共享复用。平台侧趋向引擎化，专注自动化控制逻辑，由 KCL 代码作为扩展技术外置编写业务逻辑。我们希望面对复杂的运维业务诉求，平台侧控制器逐步演进到低频变更，甚至零变更。</p><p>应用侧输入 20% 内容，以平台侧前端结构体为界面声明应用侧诉求，重点在于要什么、要做什么，所写即所得。应用侧通过面向多项目、多租户、多环境、多应用的代码工程结构组织代码，通过 Pull Request 发起变更，通过 CICD pipeline 完成白盒化的线上变更。同时，应用侧有对单应用编译、测试、验证、模拟的自由度，在充分验证后交付使用；对多应用可通过 KCL 语言能力按需灵活组合。将大规模的复杂问题拆分缩小到应用粒度，得到充分验证后按需合并，本质上是一种分治思路的实践。针对蚂蚁的实际情况，我们通过 KusionCtl 工具支持研发测试环境的执行及可视化，通过 InfraForm 产品、SiteBuilder 产品等推动线上的部署过程。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-协同配置问题模型">4. 协同配置问题模型<a class="hash-link" href="#4-协同配置问题模型" title="Direct link to heading">​</a></h2><p>理解了落地思路和场景实践方式，我们将进一步下钻拆解具体的协同场景，同时分析 KCL 语言在配置场景的设计和应用。</p><p>我们先来看平台侧编写轻量级应用级 API 的一些要点。平台侧同学可以通过单继承的方式扩展结构体，通过 mixin 机制定义结构体内属性的依赖关系及值内容，通过结构体内顺序无关的编写方式完成声明式的结构体定义，此外还支持如逻辑判断、默认值等常用功能。</p><p>对于声明式与命令式的差异做简单的分析，我们以斐波那契数列为例，可以把一组声明式代码看作一个方程组，方程式的编写顺序本质上不影响求解，而“求解”的过程由 KCL 解释器完成，这样可以避免大量命令式拼装过程及顺序判断代码，对于存在复杂依赖的结构体而言优化尤为明显。</p><p>对于复杂结构，命令式拼装的写法多出一倍以上的代码量，补丁代码使得结果难以预测，同时需要考虑执行顺序问题，特别是在模块化过程中调整存在依赖的模块顺序非常繁琐且易出错。对于各种配套能力，我们通过 mixin 机制编写，并通过 mixin 声明的方式“混入”到不同的结构体中。</p><p><img loading="lazy" src="/assets/images/15-3a5a0768baaa395c6c70fd5a59d0dd9d.png" width="2236" height="1082"></p><p>对于平台侧来说，稳定性保证尤为重要。</p><p>当配置数据量逐步增大时，良构类型是保证编译时问题发现的有效手段，KCL spec 包括了完备的类型系统设计，我们正在实践静态类型检查和推导，逐步增强类型的完备性。</p><p>同时 KCL 引入了多种不可变手段，支持用户按需定义结构体内属性的不可变性。通过这两种基础而重要的技术手段使得大量违反编写约束的情况可以在编译时被检查发现。</p><p><img loading="lazy" src="/assets/images/16-36f723413c8f7d55169049c9c2f76dec.png" width="2014" height="1076"></p><p>对于业务向的内容，KCL 支持通过结构体内置的校验规则及单元测试的方式支持。以下图所示代码为例，我们在 AppBase 中定义对 containerPort、services、volumes 的校验规则，同时在 MyProdApp 中定义叠加的环境相关的校验规则。目前校验规则在运行时执行判断，我们正在尝试通过编译时的静态分析对规则进行判断从而发现问题。</p><p><img loading="lazy" src="/assets/images/17-4c4e4ab7f1e7871e3ebe721aac60d1aa.png" width="1636" height="826"></p><p>此外对于平台侧来说，升级推进是必须面对的问题。我们首先需要考虑最坏情况，即提供给用户的前端结构体需要做不兼容的调整，按照新增配置项并下线老配置项的思路，我们需要对待下线字段进行禁用，并以合理的方式告知用户。</p><p>当平台自身出现不兼容更新时问题相似，只是需要平台侧后端结构体进行调整，应用侧用户不直接感知。KCL 针对这类问题提供了字段禁用的功能，使用被禁用字段将在编译阶段通过警告或错误的方式提示，编译错误将 block 编译，从而迫使用户在编译阶段进行修改，避免将问题带入运行时造成影响。</p><p>对于兼容的平台侧调整，通常在后端结构体修改导入的原子定义文件即可。对于 KCL 解释器自身的变化，我们通过单元测试、集成测试、模糊测试等进行验证，对于 plugin 的变更通过 plugin 自身的测试验证。KCL 解释器及 plugin 的变化通过需要 Konfig 代码库的 UT、IT 进行测试验证，保障已有代码正常工作。在经过测试验证后，发起 Pull Request 通过 code review 评审。</p><p><img loading="lazy" src="/assets/images/18-98948ced448014abb15351065bc9b0c3.png" width="1258" height="784"></p><p>我们再来简单梳理应用侧协同的场景。假设存在基线配置及生产环境配置，在我们的实践中存在三种典型场景。</p><p>第一种场景中，基线与生产配置中各定义了同名配置的一部分，由 KCL 自动合并生成最终配置块，这适用于对称配置的场景非常有效，如果出现冲突则会进行冲突报错。</p><p>第二种场景中，我们希望在生产配置中覆盖基线配置中的一些配置项，类似 Kustomize 的 overlay 覆盖功能，事实上这是大多数熟悉 Kubernetes 使用者的诉求。</p><p>对于第三种场景，编写者希望配置块全局唯一，不能进行任何形式的修改，若出现同名配置则会在编译阶段报错。在真实的场景中，基线与各环境配置可由研发与 SRE 配合完成，也可以由 Dev 独立完成，Kusion 本身不限制使用者职能。</p><p><img loading="lazy" src="/assets/images/19-e783dd554ad98c689adabf10c0c4e79a.png" width="2352" height="1040"></p><p>通过场景分析我们对 KCL 有了初步的了解，我们以编程语言的理论、技术，云原生应用场景三方面为输入设计 KCL，我们希望通过简单有效的技术手段支撑平台侧、应用侧完成基础设施描述，将问题尽可能暴露在 KCL 编译、测试阶段，以减少线上运行时的问题频次。此外我们提供了便利的语言能力和工具帮助不同的使用群体更高效的完成其工作，并通过工程化的方式组织、共享代码，对接 Kubernetes API 生态。</p><p><img loading="lazy" src="/assets/images/20-91999d9afe2c59f42c861f4b41cc9eed.png" width="2252" height="1152"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-抽象模型">5. 抽象模型<a class="hash-link" href="#5-抽象模型" title="Direct link to heading">​</a></h2><p>通过对 Kusion 落地集成、协同编程场景的分析，我们了解到 Kusion 技术的组成场景及使用方式。我们再来看看 Kusion 关键抽象模型。</p><p>我们先来看 KCL 代码的抽象模型。以下图为例，首先 KCL 代码在编译过程中形成两张有向无环图，分别对应结构体内部声明代码及结构体使用声明。编译过程可以简单分为展开、合并、代换三步。通过这样的计算过程，在编译时完成了大部分代换运算，最终运行时进行少量计算即可得到最终的解。在编译过程中，我们同步进行类型检查和值的检查，他们的区别是类型检查是做泛化，取偏序上确界，值检查是做特化，取偏序下确界。 &gt;</p><p><img loading="lazy" src="/assets/images/21-e6504c15635929d9abf205ca849bc1da.png" width="1836" height="1146"></p><p>对于 KCLVM 的解释器，我们采用了标准的分层解耦的设计方式，由 parser、compiler、VM 三部分组成。我们希望尽可能的在编译时完成工作，例如图的展开、代换，类型的检查、推导等，这样可以保持 VM 部分尽可能简单。后续我们将在 KCLVM compiler 中支持对 WASM 中间表示的编译支持。此外我们通过 plugin 机制支持对 VM 运行时能力的扩展，并考虑了对 LSP Server 的支持以降低 IDE、编辑器支持成本。</p><p><img loading="lazy" src="/assets/images/22-a36339df8cee3c1d33beea9f655c2848.png" width="1278" height="588"></p><p>在工程化方面，我们通过 project、stack、component 三级方式组织 KCL 代码。当代码映射到 Kubernetes 集群时，Kusion 支持两种映射方式。</p><p>第一种方式支持将 stack 映射为 namespace，component 在 namespace 内存在，即 stack 内共享资源配额，component 间通过 SDN 及 Mesh 能力做隔离，这是社区比较常见的一种实践方式。</p><p>第二种方式将 component 映射为 namespace，stack 通过 label 标识，通过 SA 管理权限，资源配额定义在 component 维度，component 间通过 namespace 的隔离能力做隔离，这是蚂蚁目前线上环境的实践方式。无论如何映射，用户无需感知物理集群对接及切换细节。此外，KCL 代码中资源定义都可以通过唯一的资源 ID 定位，这也是对代码进行“增删改查”的基础。</p><p><img loading="lazy" src="/assets/images/23-d86a1f8e507b90c43a7d28133575af6b.png" width="1628" height="1052"></p><p>为了支持上述的隔离及映射逻辑，我们提供了 KusionCtl 工具帮助用户完成项目结构初始化、Kubernetes 集群映射、执行状态跟踪及展示、Identity 权限集成等常用功能。用户可以通过 KusionCtl 完成研发、测试环境的执行和验证工作。</p><p><img loading="lazy" src="/assets/images/24-d351dc8a680bb652a1996162397cab27.png" width="968" height="1158"></p><p>对于线上环境，我们更推荐使用基于 Kusion 的运维产品进行变更操作。我们希望通过 KCL 代码开放、透明、声明式、意图导向、分层解耦的定义基础设施，本质上是面向数据及其约束的一种协同工作，变更是一种数据的流动。我们通过前置的预编译、计算、验证，最终将数据交付到各环境的运行时，相比于经典命令式系统中计算逻辑流动的方式，可以最大程度避免复杂命令式计算造成的运行时数据错误，特别是当计算逻辑发生变更时，这种运行时计算错误的结果通常都是一次线上故障。</p><p>最后，我们来看一种 Kusion 思路的技术架构，我们仍然以控制器、业务层、编排层、任务及管道的分层逻辑来看。自下而上的，由 Kubernetes API Server 收敛了管道并提供了原生资源定义，并通过 CRD &amp; Operator 进行扩展提供稳定的原子任务定义。从我个人的角度看，Operator 如其名约“操作员”，重复着接收订单、执行操作的简单循环，订单未完成则持续操作。</p><p>Operator 应尽可能保持简单，避免复杂的业务逻辑拆解、控制逻辑、状态机，同时避免因为微小的差异创建新的 Operator 或通过 Operator 做单纯的数据、YAML 转换。Operator 作为收敛基础设施原子能力的存在，应尽量内聚、稳定。在业务层、编排层，我们通过 KCL 代码在 Konfig 仓库中编写，并结合 GitOps 支持应用粒度的变更、编译、测试、验证。控制器层高度引擎化，聚焦自动化逻辑，根据业务场景需要定制控制器及 GUI 产品界面。应用的配置代码“组件”由多个控制器共享复用，例如建站、发布、部分运维都将依赖应用 AppConfiguration 配置代码块。</p><p><img loading="lazy" src="/assets/images/25-bb23856fe0a42527bee9c3bc0626872d.png" width="1546" height="1160"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-总结--展望">6. 总结 &amp; 展望<a class="hash-link" href="#6-总结--展望" title="Direct link to heading">​</a></h2><p>最后，我们对开放协同技术工作做一个总结。</p><p>我们常说 Kubernetes 是云计算的 Linux/Unix，相比于 Unix 丰富的外围配套生态，Kubernetes 在配套技术能力上还有很长的路径。对比于使用便利的 Shell、Tools，我们还缺少一种符合 Kubernetes 声明式、开放、共享设计理念的语言及工具，Kusion 希望能在这一领域有所帮助，提升基础设施的开放程度及使用效率，易于共享、协同，提升稳定性，简化云原生技术设施的接入方式。</p><p><img loading="lazy" src="/assets/images/26-d18442d2d0183911f70f69bda0e16ad4.png" width="1004" height="530"></p><p>我们的探索和实践仍然在一个初级阶段，我们希望通过 Kusion 的技术和服务能力在运维、可信、云原生架构演进方面起到积极的作用。</p><p>我们希望推进真正的基础设施代码化，促成跨团队的 DevOps，成为持续部署与运维的技术支撑。在可信方面，策略及代码、可信集成、标准化的支撑是我们后续的工作重点之一，特别是与策略引擎的结合，是开放可信技术能力的关键步骤。</p><p>在云原生架构方面，我们将持续推进架构现代化的演进，通过技术手段支持更多上层自动化产品业务的快速创新，同时通过统一的流程、企业级的技术能力支持服务好基础设施应用场景。</p><p><img loading="lazy" src="/assets/images/27-f506606d2e5ab41b4c930f2a2ebe5287.png" width="1564" height="834"></p><p>纵观历史，技术总是朝着提高整体社会协作效能演进。 Kusion 带来的云原生开放协同无疑是这条朴素规律再次发挥效力的注脚。</p>]]></content>
        <author>
            <name>朵晓东</name>
        </author>
        <category label="kusion" term="kusion"/>
    </entry>
</feed>