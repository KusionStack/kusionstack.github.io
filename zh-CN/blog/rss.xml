<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>KusionStack Blog</title>
        <link>https://kusionstack.io/zh-CN/blog</link>
        <description>KusionStack Blog</description>
        <lastBuildDate>Mon, 19 Sep 2022 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[KusionStack Origin, present and future]]></title>
            <link>https://kusionstack.io/zh-CN/blog/2022-origin-present-and-future</link>
            <guid>2022-origin-present-and-future</guid>
            <pubDate>Mon, 19 Sep 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[KusionStack Origin, present and future]]></description>
            <content:encoded><![CDATA[<p><a target="_blank" href="/zh-CN/assets/files/KusionStack-origin-present-and-future-352c9a718f58d3aa0a6f123dae46a409.pdf"><img loading="lazy" alt="KusionStack Origin, present and future" src="/zh-CN/assets/images/KusionStack-origin-present-and-future-c678397b18a35d71fe4661a12eb16666.png" width="2636" height="1476"></a></p><p><a href="https://kusionstack.io/talks/KusionStack-origin-present-and-future.pdf" target="_blank" rel="noopener noreferrer">Download PDF</a></p>]]></content:encoded>
            <category>KusionStack</category>
            <category>Kusion</category>
        </item>
        <item>
            <title><![CDATA[从规模化平台工程实践，我们学到了什么]]></title>
            <link>https://kusionstack.io/zh-CN/blog/2022-lean-from-scale-practice</link>
            <guid>2022-lean-from-scale-practice</guid>
            <pubDate>Fri, 16 Sep 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[摘要：本文尝试从平台工程、专用语言、分治、建模、自动化和协同文化等几个角度阐述规模化平台工程实践中的挑战和最佳实践。希望通过把我们平台工程的理念和实践分享给更多企业和团队，一起让一些有意思的变化发生。]]></description>
            <content:encoded><![CDATA[<p><strong>摘要</strong>：本文尝试从平台工程、专用语言、分治、建模、自动化和协同文化等几个角度阐述规模化平台工程实践中的挑战和最佳实践。希望通过把我们平台工程的理念和实践分享给更多企业和团队，一起让一些有意思的变化发生。</p><p>本文基于 <a href="https://kusionstack.io/docs/user_docs/intro/kusion-intro" target="_blank" rel="noopener noreferrer">KusionStack</a> 技术栈在蚂蚁平台工程及自动化中的实践总结而成。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-平台工程让企业级-devops-发生">1. 平台工程：让企业级 DevOps 发生<a class="hash-link" href="#1-平台工程让企业级-devops-发生" title="Direct link to heading">​</a></h2><p>DevOps 理念在 10 多年前被提出，从 KVM 到容器再到云原生时代，大量企业投入 DevOps 运动以期望解决内部规模化运维效率和平台建设效率的困境。其中大部分陷入过某种基于对 DevOps 朴素认知的 Anti-Pattern，同时也有部分公司探索出自己的路径。我经历过如下图简示的 Anti-Patterns，Dev 与 Ops 团队各行其是，或者简单的强制 Dev 团队独立完成 Ops 工作。在<a href="https://web.devopstopologies.com/#anti-types" target="_blank" rel="noopener noreferrer">这里</a>可以找到更多更典型分类。</p><p><img loading="lazy" src="/zh-CN/assets/images/devops-anti-pattern-a9159a518bb8f608b79a505dad7f2aaa.png" width="988" height="410"></p><p>企业内规模化 DevOps 难以推行的原因多种多样，特别是在企业内自持基础设施、同时采用云上技术平台的公司阻力最大。其中以这几种情况尤为常见：</p><ul><li>研发团队和运维团队由于部门墙、领导者缺少洞察等等原因各自为政，难以达成一致意见</li><li>研发团队低估了基础设施技术、运维、稳定性工作的专业性、复杂性和快速变化，以朴素的 DevOps 理解强制应用研发者成为专家</li><li>领导者建立了专职的 DevOps 团队，但沦为中间的执行者，没能让 Dev 和 Ops 团队各自向前一步，紧密协同</li><li>平台研发团队对规模化带来的业务复杂性以及技术演进带来的技术复杂性应对不足，无法对应用研发者提供有效的技术支撑</li><li>...</li></ul><p>不同于面向云上托管基础设施服务和 DevOps-as-a-Service 产品工作的小型团队，中大型企业往往需要根据自身团队架构和文化建立适当的 DevOps 体系。从成功案例看，无论是 Meta 公司由 Dev 完全承担 Ops 职能，还是 Google 公司引入 SRE 团队作为中间层，平台工程（<a href="https://platformengineering.org/blog/what-is-platform-engineering" target="_blank" rel="noopener noreferrer">Platform Engineering</a>）都扮演了非常重要的角色。平台工程旨在帮助企业构建面向应用研发者的自服务运维体系，尝试通过工程化的技术手段和工作流程解决以下关键问题：</p><ul><li>设计合理的抽象层次，帮助应用研发者降低对 Infra、platform 等技术以及运维、稳定性工作的认知负担</li><li>为应用研发者提供统一的工作界面和空间，避免用户陷入割裂的平台产品界面、复杂的工作流中</li><li>帮助研发者通过有效的工作流程和推荐路径基于 <a href="https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/" target="_blank" rel="noopener noreferrer">内部工程平台</a> 快速开展工作</li><li>帮助研发者通过配套的 CI、CD、CDRA 等产品自服务管理应用生命周期</li><li>帮助平台产品研发团队简单、高效、一致的开放其平台基础能力</li><li>通过培训、布道、运营等手段营造协同工作和分享的文化</li></ul><p>事实上，不是所有人都应该或者能够成为这个领域的专家，这非常困难！实际上平台技术团队的专家通常也仅擅长自己的专业领域而已，特别是在云原生理念和技术广泛应用的今天，面向大量高度开放、可配置的平台技术带来的成百上千的应用配置，PaaS 领域的业务复杂性，以及高稳定性和统一治理的要求，而平台工程的目的正是为了让应用研发者尽可能简单无痛的参与到这样规模化的 DevOps 工作中。在蚂蚁的实践中，我们更趋向于以下这种合作状态，在团队架构和工作模式上更靠近 Google 的最佳实践。平台研发者及 SRE 成为 “Enabler” 支持应用研发者自服务的完成研发及交付运维，同时应用研发者使其应用可交付运维的工作结果也成为运维人员可以接手应用运维工作的基础。最终 SRE、应用研发及运维人员把工作过程中的问题和痛点反馈给平台研发者形成正向循环。</p><p><img loading="lazy" src="/zh-CN/assets/images/devops-cycle-8923aa81a904e40ee30ede0c0d4c16b2.png" width="694" height="666"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-专用语言工程化方式的一极">2. 专用语言：工程化方式的一极<a class="hash-link" href="#2-专用语言工程化方式的一极" title="Direct link to heading">​</a></h2><p>有什么比一种专用语言更适合开放的、自服务的、面向领域业务的问题定义，同时需要满足自动化、低安全风险、低噪音、易治理的企业内部要求吗？正如记录音乐有五线谱，存储时间序列数据有时序数据库一样，在平台工程的特定问题域内，一批配置和策略语言用于编写和管理规模化复杂配置及策略。不同于混合编写范式、混合工程能力的高级通用语言，这类专用语言的核心逻辑是以收敛的有限的语法、语义集合解决领域问题近乎无限的变化和复杂性，将规模化复杂配置和策略编写思路和方式沉淀到语言特性中。</p><p>在蚂蚁的平台工程实践中，我们强化了客户端的工作方式，将围绕应用运维生命周期的模型、编排、约束和策略稳定、可扩展的通过专用语言  <a href="https://github.com/KusionStack/KCLVM" target="_blank" rel="noopener noreferrer">KCL</a>  编写维护在共享仓库 <a href="https://github.com/KusionStack/konfig" target="_blank" rel="noopener noreferrer">Konfig</a> 中。 KCL 是一种面向有编程能力的应用研发者的静态强类型语言，提供现代高级语言的编写体验和围绕领域目的有限功能。在平台工程实践中 KCL 不是一种仅用于编写 K-V 对的语言，而是一种面向平台工程领域的专用语言。应用研发者、SRE、平台研发者面向 Konfig 协同研发，通过 KCL 原生功能编写应用配置，以及在 PaaS 领域更为高频和复杂的<a href="https://kusionstack.io/docs/reference/lang/lang/tour/#schema" target="_blank" rel="noopener noreferrer">模型</a>抽象、<a href="https://kusionstack.io/docs/reference/lang/lang/tour/#function" target="_blank" rel="noopener noreferrer">功能函数</a>和<a href="https://kusionstack.io/docs/reference/lang/lang/tour/#validation" target="_blank" rel="noopener noreferrer">约束</a><a href="https://kusionstack.io/docs/reference/lang/lang/tour/#rule" target="_blank" rel="noopener noreferrer">规则</a>，即编写稳定、可扩展的业务模型、业务逻辑、防错约束和环境规则。Konfig 仓库则成为统一的编程界面，工作空间和业务层载体，而基于 KCL 的安全、低噪音、低副作用、统一的编写范式更有利于长期管理和治理。</p><p><img loading="lazy" src="/zh-CN/assets/images/kcl-dev-f53b8dffa98db3aeba220f1403c6eaed.png" width="1500" height="910"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-分治解构规模化问题">3. 分治：解构规模化问题<a class="hash-link" href="#3-分治解构规模化问题" title="Direct link to heading">​</a></h2><p>分治思路是解决规模化问题的钥匙，从 MapReduce 到 Kubernetes 无不体现其功效。在规模化交付运维领域，经典运维平台试图在统一的黑盒平台产品中，以内置的统一模型、编排、provision 技术来应对全量业务场景。这样的实践可以快速启动，在小范围内奏效，但随着不同业务主体采用率提升引入差异化需求，同时随着持续变化的平台技术逐渐进入疲态。</p><p><img loading="lazy" src="/zh-CN/assets/images/classic-plats-0e5580742fe5fb7b9d832a1dc235eb36.png" width="1500" height="901"></p><p>在蚂蚁的实践中，Konfig  monorepo 是内部工程平台向研发者开放的编程界面和工作空间，帮助应用研发者以统一的编程界面编写围绕应用运维生命周期的配置和策略，从而编排和使用存量和新增的平台基础设施，按需创建管理云原生环境以及基于 RBAC 的权限，并通过 GitOps 方式管理交付过程。Konfig monorepo 为不同场景、项目、应用提供了独立的白盒的编程空间，其内生的扩展性来源于：</p><ul><li>灵活、可扩展、独立的客户端的 <a href="https://kusionstack.io/docs/user_docs/concepts/project-stack" target="_blank" rel="noopener noreferrer">工程结构设计</a></li><li>独立配置块 <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#-operators-1" target="_blank" rel="noopener noreferrer">自动合并技术</a>支持任意分块、可扩展的配置块组织</li><li><a href="https://kusionstack.io/docs/reference/lang/lang/tour/#type-system" target="_blank" rel="noopener noreferrer">静态类型系统</a>技术提供现代编程语言可复用、可扩展的类型化建模和约束功能</li><li>项目粒度的 GitOps CI 工作流程定义支持</li><li>基于 Kusion 引擎的 provision 技术选择</li></ul><p>Konfig monorepo 提供了分治的、可组合的工程结构设计、代码组织、建模方式、工作流程定义和 provision 技术选择支持，同时又以一致的研发模式和工作流承载了可扩展的业务需求。这样客户端的工作方式在保证灵活性、可扩展性、可移植性的同时也降低了对服务端扩展机制，如 Kubernetes API Machinery，持续增长的压力。</p><p>下图示意了一种 Konfig  monorepo 中 GitOps 方式的典型的自动化工作流程，从面向应用的代码变更开始，通过可配置的 CI、CD 过程触达运行时，这样的机制相比中心化的黑盒产品方式更加开放、可定制、可扩展，也免去了针对不同业务场景、不同项目、应用设计笨拙的配置文件管理 portal 的必要。</p><p><img loading="lazy" src="/zh-CN/assets/images/d-c-overview-0d3ba9e610b9a7497e5044e7de60124d.png" width="1500" height="985"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-建模边际收益和长尾问题">4. 建模：边际收益和长尾问题<a class="hash-link" href="#4-建模边际收益和长尾问题" title="Direct link to heading">​</a></h2><p>有了分治的白盒化的工程结构设计、代码组织方式、建模方式、工作流程定义和 provision 技术选择，以怎么的策略面向平台 API 工作是另一个需要考虑的问题。在企业内典型的争议在于直面平台细节还是设计一种抽象，或者上升到显式（explicit）和隐式（implict）的理念的争议。</p><p>抽象的隐式的方式是运维平台工程师们面向非专家型终端用户的普遍选择，他们希望能设计出易于理解、便于使用的应用模型或 Spec 抽象，与具体的平台技术细节隔离，降低用户认知负担，并通过降低细节感知防错。但大部分运维平台的研发者倾向于设计一种强大的、统一的应用模型或 Spec 抽象，在实践中往往会遇到这些阻碍：</p><ul><li>随着企业内不同业务主体采用率的提升，统一建模难以落地。在蚂蚁内部最典型的案例是 Infra 基础技术类组件和 SaaS 应用间的巨大差异性，SaaS 应用便于统一，Infra 应用往往需要单独设计。</li><li>面向企业内大量的平台技术，统一模型自身难以稳定，特别是应对持续变化的业务需求和平台技术驱动的需求增长。在蚂蚁的实践中，交付运维受多种因素影响有较强的不稳定性，同时围绕应用的 deliverable、runtime、security、instrumentation 的业务需求也在增长。以 instrumentation 为例，近两年对应用运行时可观察性、SLO 定义的需求快速增长直接驱动了终端用户使用的变化。</li><li>抽象模型的共性问题是需要面向用户设计出合理的模型，面向平台 API 细节保持同步。</li></ul><p>在蚂蚁的实践中，面向终端用户即应用研发者我们采用了抽象模型的方式，通过如下思路解决几个关键问题：</p><ul><li>面向典型应用场景（如蚂蚁的 Sofa 应用）建模，这些模型由平台研发者、平台 SRE 主导开发，与应用研发者共同维护，达到用户体验、成本和标准兼容的平衡，在蚂蚁的实践中抽象模型的信息熵收敛比约为 1：5，通过广泛的高频使用保证建模投入的边际收益。</li><li>对于非典型用户场景或应用，由平台研发者、平台 SRE 支持应用研发者完成针对应用的模型设计。KCL <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#schema" target="_blank" rel="noopener noreferrer">schema</a> 和 <a href="https://kusionstack.io/docs/reference/lang/lang/tour#protocol--mixin" target="_blank" rel="noopener noreferrer">mixin</a> 等机制帮助用户建模、抽象、继承、组合、复用，减少重复代码，事实上这样的建模设计工作也是应用 PaaS 领域的重点之一。最终大量 “非标” 平台技术在蚂蚁内部首次以一致的方式被纳管，有效解决了长尾问题。在典型协同模式下，平台研发者、平台 SRE 编写平台能力基础组件成为 “Enabler”，帮助应用研发者使用平台能力基础组件快速“搭积木”，完成其应用模型的研发工作。</li><li>面向平台技术，我们提供了平台 API Spec 到 KCL 类型代码的<a href="https://kusionstack.io/docs/reference/cli/openapi/" target="_blank" rel="noopener noreferrer">生成技术</a>，并通过<a href="https://kusionstack.io/docs/reference/lang/lang/tour/#multi-file-compilation" target="_blank" rel="noopener noreferrer">组合编译技术</a>原生支持对不同 Kubernetes API 版本的编译时选择，在内部实践中解决了应用抽象模型面向不同版本 Kubernetes 集群工作的灵活需求。同时，KCL 支持 <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#validation" target="_blank" rel="noopener noreferrer">in-schema 约束</a>和独立环境<a href="https://kusionstack.io/docs/reference/lang/lang/tour/#rule" target="_blank" rel="noopener noreferrer">规则</a>的编写。此外，KCL 还提供了 <a href="https://kusionstack.io/docs/reference/lang/lang/tour/#decorators" target="_blank" rel="noopener noreferrer">deprecated 装饰器</a>支持对已下线模型或模型属性的标注。通过在客户端健壮的、完备的模型和约束机制，在编译时暴露如配置错误、类型漂移等常见问题。相对于运行时左移的发现问题，避免推进到集群时发生运行时错误或故障，这也是企业内，特别是高风险等级企业，对生产环境稳定性的必须要求。</li></ul><p>对于基础平台技术的专家型用户，他们通常非常熟悉特定的技术领域，更希望以直面平台细节的显式的方式工作，语言提供必要的动态性和模块化支持，通过类型和约束机制保证稳定性。但这种显式的方式无法解决专家用户不熟悉跨领域平台技术使用细节的问题，也不能解决面向平台技术的扩展性和复杂性叠加的问题。在蚂蚁内部小范围基于 YAML 的显式的工程实践中，面向大量高度开放、可配置的平台技术，复杂性随着平台技术使用率持续叠加，最终陷入难以阅读、编写、约束、测试及维护的僵化状态。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-自动化新的挑战">5. 自动化：新的挑战<a class="hash-link" href="#5-自动化新的挑战" title="Direct link to heading">​</a></h2><p>运维自动化是基础设施运维领域的经典技术范畴，随着云原生理念及技术的推波助澜，可以被自动化集成成为企业运维实践的基本要求，开源开放、高度可配置的 CI、CD 技术逐步被企业采纳，黑盒的、无法被集成的 “产品” 方式逐步被灵活的可编排方式弱化并替代。这种实践的主要优势在于其强大的自定义编排和链接能力，高度的可扩展性和良好的可移植性。特别是在 Kubernetes 生态，GitOps 方式有更高的采用率，与可配置的 CI、CD 技术有天然的亲和性。这样的变化也在推进以工单和运维产品为中心的工作流逐步转变为以工程效率平台为中心的自服务工作流，而生产环境的运维能力则成为了工作流中面向生产自动运维的一个重要环节。在开源社区，面向不同研发效率平台的抽象层技术创新也在活跃进行中，平台侧研发者希望通过最短的认知和实践路径打通应用到云环境的 CI、CD 过程。</p><p>在蚂蚁的工程实践中，工程效率平台深度参与了 Konfig monorepo 的开放自动化实践，我们的实践方向也与工程效率平台技术演进方向高度一致。在从几人到几十人再到几百人的协同工作中，面向运维场景的工作流设计，高频的代码提交和 pipelines 执行，实时自动化测试和部署过程，这些对服务于单库的工程效率平台造成了很多的挑战。特别是 monorepo 中多样化的业务需要独立且强大的工作流自定义和操作支持，也需要高实时性、强 SLO 保障的并行的工作流执行能力，这些需求与单库模式的需求有巨大的差异，也给我们制造了很多麻烦。大部分配置语言是解释型语言，而 KCL 被设计为一种编译型语言，由 Rust、C、LLVM 优化器实现，以达到对规模化 KCL 文件提供高性能编译和运行时执行的目标，同时支持编译到本地码和 wasm 以满足不同运行时的执行要求。另外 Git 的存储及架构设计不同于 <a href="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext" target="_blank" rel="noopener noreferrer">Citc/Piper</a> 架构，不适用于规模化代码的 monorepo，所幸今天我们的代码量还没有遇到很大的问题。我们正在一起工作解决这些问题，希望随着实践的深入逐步解决他们。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-协同和文化更重要的事">6. 协同和文化：更重要的事<a class="hash-link" href="#6-协同和文化更重要的事" title="Direct link to heading">​</a></h2><p>以上的技术、工具、机制都非常重要，但我必须要说，对于工程化、Devops 更重要的是团体与团队的协同、合作和分享的文化，因为这是一种由人组成的工作，人和文化是其中的关键。在企业内，如果部门墙、团队壁垒丛生，流行封闭糟糕的工程文化，我们通常会看到大量私有的代码库和私有文档，小群体的判断和工作方式，本该紧密合作的团队以各自目标为导向各行其是，在这样的文化下我认为一切规模化工作都会非常困难。所以如果你所在的公司或团队想采纳规模化 Devops，我认为最重要的是做好广泛的沟通并开始文化的建设，因为这绝对不只是几个人的事，并且这很有难度且不可控。</p><p>在蚂蚁的实践中，初期总有各种各样的困难，大家对自服务机制和协同文化的担心尤为突出，例如 “我居然要写代码？” “我的代码居然跟其他团队在一个仓库里？” ，“我负责的工作可不简单，这种方式行不通” 都是很典型的担忧。所幸我们最终建立了一个面向共同目标的虚拟组织，合作方和领导者给予了充分的支持，我们在理念和工作方式上达成一致并协同工作。在实践过程中，大多数工程师并不是障碍，当然他们会吐槽技术、流程和机制还不够完善，希望获得更好的体验，这无可厚非。真正的障碍首先来自于运维平台研发团队自身，我看到一些公司的 Devops 理想最终回归到运维平台团队替代应用研发者做掉所有工作，甚至不让用户接触到代码和工具链这些生产工具，急于桥接到原有的 ClickOps GUI 界面，我认为这跑偏了，也低估了用户自身的能力和创造力。另外障碍也来自于部分平台技术团队的技术负责人，他们很难放下持续多年的已有工作，难以接受转向到新的用户服务模式。可行的办法是让他们明白这项工作的意义和远景，逐步的分阶段的影响他们。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="7-小结">7. 小结<a class="hash-link" href="#7-小结" title="Direct link to heading">​</a></h2><p>经过一年多的实践，有 400+ 研发者直接研发参与了 Konfig monorepo 的代码贡献，管理了超过 1500 个 projects，其中平台研发者及平台 SRE 与应用研发者比例不到 1：9，这些应用研发者有些是应用 owner 本人，有些是应用研发团队的代表，这由应用团队自己决定。通过持续的自动化能力搭建，基于 Konfig monorepo 每天发生 200-300 次 commits，其中大部分是自动化的代码修改，以及大约 1k pipeline 任务执行和近 10k KCL 编译执行。在今天如果将 Konfig 中全量代码编译一次并输出会产生 300W+ 行 YAML 文本，事实上一次发布运维过程中需要多次不同参数组合的编译过程。通过轻量化，便于移植的代码库和工具链，我们完成了一次意义重大的外部专有云交付，免去了改造、移植输出一系列老旧运维平台的痛苦。在蚂蚁内部我们服务了几种不同的运维场景，正在扩大应用规模并探索更多的可能性。</p><p>最后我想说一说下一步的计划，我们的技术和工具在易用性和体验上还有很大的提升空间，需要更多的用户反馈和持续的改进，体验工作没有快速路径。在测试方面，我们提供了简单的集成测试手段，起到了冒烟测试的作用，但这还不够，我们正在尝试基于约束、规则而非测试的方式保证正确性。在工作界面方面，我们希望构建基于 IDE 的线下工作空间，持续规约、优化内部线上产品体验和工作流程。同时我们希望持续提升覆盖范围和技术能力。另外我们也希望将实践方式更广泛的应用在 CI 构建，自动化运维等场景，缩短终端用户的信息感知和端到端工作流程。目前KusionStack还处于刚刚开源的非常早期的阶段，在未来还有大量的工作要做。最重要的是我们希望把我们平台工程的理念和实践分享给更多企业和团队，一起推动并见证一些有意思的变化发生。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="8-引用">8. 引用<a class="hash-link" href="#8-引用" title="Direct link to heading">​</a></h2><ul><li><a href="https://kusionstack.io/docs/user_docs/intro/kusion-intro" target="_blank" rel="noopener noreferrer">https://kusionstack.io/docs/user_docs/intro/kusion-intro</a></li><li><a href="https://platformengineering.org/blog/what-is-platform-engineering" target="_blank" rel="noopener noreferrer">https://platformengineering.org/blog/what-is-platform-engineering</a></li><li><a href="https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/" target="_blank" rel="noopener noreferrer">https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/</a></li><li><a href="https://web.devopstopologies.com/#anti-types" target="_blank" rel="noopener noreferrer">https://web.devopstopologies.com/#anti-types</a></li><li><a href="https://github.com/KusionStack/KCLVM" target="_blank" rel="noopener noreferrer">https://github.com/KusionStack/KCLVM</a></li><li><a href="https://kusionstack.io/docs/reference/lang/lang/tour/#%E9%85%8D%E7%BD%AE%E6%93%8D%E4%BD%9C" target="_blank" rel="noopener noreferrer">https://kusionstack.io/docs/reference/lang/lang/tour</a></li><li><a href="https://kusionstack.io/docs/user_docs/concepts/project-stack" target="_blank" rel="noopener noreferrer">https://kusionstack.io/docs/user_docs/concepts/project-stack</a></li><li><a href="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext" target="_blank" rel="noopener noreferrer">https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext</a></li></ul>]]></content:encoded>
            <category>KusionStack</category>
            <category>Kusion</category>
        </item>
        <item>
            <title><![CDATA[KusionStack 开源有感]]></title>
            <link>https://kusionstack.io/zh-CN/blog/2022-sense-of-open-day</link>
            <guid>2022-sense-of-open-day</guid>
            <pubDate>Tue, 07 Jun 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[历时两年，打破“隔行如隔山”困境]]></description>
            <content:encoded><![CDATA[<p><strong>历时两年，打破“隔行如隔山”困境</strong></p><p>本文撰写于 KusionStack 开源前夕，作者有感而发，回顾了团队从 Kusion 项目开发之初到现今成功走上开源之路的艰辛历程。当中既描述了作者及其团队做 Kusion 项目的初心和项目发展至今的成果，也表达了作者自身对团队的由衷感激，字里行间都散发着真情实感。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-kusionstack-是什么">1. KusionStack 是什么？<a class="hash-link" href="#1-kusionstack-是什么" title="Direct link to heading">​</a></h2><p>KusionStack 是开源的可编程云原生协议栈！</p><p>Kusion 一词来源于 fusion（意为融合），希望通过一站式的技术栈融合运维体系的多个角色，提升运维基础设施的开放性、扩展性，从整体上降本增效。KusionStack 通过定义云原生可编程接入层，提供包括配置语言 KCL、模型界面、自动化工具、最佳实践在内的一整套解决方案，连通云原生基础设施与业务应用，连接定义和使用基础设施的各个团队，串联应用生命周期的研发、测试、集成、发布各个阶段，服务于云原生自动化系统建设，加速云原生落地。</p><p><img loading="lazy" src="/zh-CN/assets/images/1-e6d5f19b79120ca8410c3f8c902c3e29.jpg" width="1080" height="720"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-为了一个理想的运维体系">2. 为了一个理想的运维体系<a class="hash-link" href="#2-为了一个理想的运维体系" title="Direct link to heading">​</a></h2><p>2019 年秋，MOSN 的工作已持续了近两年，期间我们逐步完成了在支付宝核心链路的形态验证。整个过程中除了 MOSN 本身面对的种种技术挑战和困难，所谓的云原生技术红利，实际上也已经掣肘于运维系统固化所造成的效率制约。</p><p>有一天主管找我吃饭（下套），期间向我描述了他理想中的运维体系：</p><p>他希望 SRE 能通过一种专用语言来编写需求，通过写代码来定义基础设施的状态，而不是花费极大的精力在检查、发现、修复的循环上。基础设施团队则通过提供开放的可编程语言和工具支撑不同诉求的 SRE 团队，达到更高的整体 ROI。</p><p>我立刻意识到这和 Hashicorp 的 Terraform 神之相似（后来 Hashicorp 在 2021 年底上市，以超过 150 亿美元的市值成为迄今为止市值最高的一次开源 IPO）。另一方面，不同于 IaaS 交付场景，蚂蚁面对着大量更规模化、复杂度更高的云原生 PaaS 场景，又让我想到了 Google 内部运用专用语言、工具等技术开放 <a href="https://pdos.csail.mit.edu/6.824/papers/borg.pdf" target="_blank" rel="noopener noreferrer">Borg</a> 和相关的 <a href="https://sre.google/workbook/configuration-specifics" target="_blank" rel="noopener noreferrer">运维能力的实践</a>，当时感觉这是 <a href="https://queue.acm.org/detail.cfm?id=2898444" target="_blank" rel="noopener noreferrer">一个既有意思又有挑战的事</a>。</p><p>饭桌上我们聊了一些思路以及一些还不太确定的挑战，他问我想不想搞一个试试，搞不成也没关系。当时没想太多，饭没吃完就答应了。</p><p><img loading="lazy" src="/zh-CN/assets/images/2-a982a0478af00482863c7214c62d721a.jpg" width="1080" height="720"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-漫长的学习探索与实践">3. 漫长的学习、探索与实践<a class="hash-link" href="#3-漫长的学习探索与实践" title="Direct link to heading">​</a></h2><p>隔行如隔山。</p><p>没有过语言设计研发的经验，也没有过开放自动化系统设计的经验，项目开展之初，我们就陷入了举步维艰的困境。</p><p>经历了一段漫长时间的学习、摸索和实践的反复循环之后，项目依旧没有大的起色，更困难的是我们不但要面对蚂蚁内部复杂又耦合的场景和问题，还要经受「这种高度工程化的方式在蚂蚁是否有生存土壤」的质疑。</p><p>屋漏偏逢连夜雨，期间又令人惋惜且无奈的经历了一些人事变化，同时由于种种原因，项目一度陷入了各种困境。整个 2020 年，我们在未知、纠结、无奈中度过…… </p><p>感谢瓴熙、庭坚和我的主管，感谢你们当时没有放弃这个项目，依然与我一同坚守。</p><p><img loading="lazy" src="/zh-CN/assets/images/3-1830e015242dc6cdcc1335e65ce85ca6.jpg" width="1080" height="720"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-痛并快乐的孵化之旅">4. 痛并快乐的孵化之旅<a class="hash-link" href="#4-痛并快乐的孵化之旅" title="Direct link to heading">​</a></h2><p>通过持续地布道、交流和沟通，我们逐步在基础设施技术团队和 SRE 团队找到了更多有共识的朋友。</p><p>同时在技术上，我们亦脱离了迷茫，真正意义上地启动了 Kusion 项目，也成功地从 PoC 过渡到了 MVP 的阶段。</p><p>最终，我们以“非标”应用为切入点，开始了痛并快乐着的孵化之旅。</p><p>感谢零执、青河、子波、李丰、毋涯、向野、达远……在这里无法一一列举，感谢你们的坚持让这个想法逐步成为现实。</p><p><img loading="lazy" src="/zh-CN/assets/images/4-f7895e32efe0875e8fab8e6711a5a6dd.jpg" width="1243" height="829"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-突破与进展">5. 突破与进展<a class="hash-link" href="#5-突破与进展" title="Direct link to heading">​</a></h2><p>略过中间的种种探索和实践，回顾这段历程，在这一年多的时间里我们结合了编译技术、运维及平台技术，成功建立了一个基于 Kusion 可编程技术栈的运维体系。</p><p>在业务场景上，项目覆盖了从 IaaS 到 SaaS 的大量运维场景，截至目前共接入了 800+ 应用，覆盖 9 个 BG，21 个 BU，其中典型案例交付运维提效 90% 以上，这也是蚂蚁内部第一次将大量异构应用纳入到一整套运维技术栈。</p><p>在蚂蚁我们基于云原生容器和微服务技术深入探索了 DevOps、CICD 实践，完善了蚂蚁的云原生技术体系，逐步释放了云原生效率红利，同时形成了一个近 300 人的虚拟运维研发团队。</p><p>不同职能不同团队的参与者凝聚在一起解决各自所面对的问题，贡献了 30K+ commit 和 350K+ 行代码，有一些参与者自发成为 Kusion 的研发者 。我认为这些工程师文化理念和领域知识的积累带来了远超运维业务本身的价值。</p><p><img loading="lazy" src="/zh-CN/assets/images/5-3bff5c659fcb099cfbc618a7fdfe781f.png" width="1080" height="794"></p><p>此外，Kusion 也成为了可编程基线产品、云原生运维产品、多云交付产品等新一代运维产品的基础技术，成为蚂蚁运维体系架构升级的一部分。</p><p>不忘初心，我们希望通过技术手段促进与运维参与方的合作关系的合理化、基于开放技术栈的自动化，以及运维数据与知识的沉淀积累，以达到整体协作运维效率的不断提升。</p><p>同时，因蚂蚁内部运维场景较多且链路复杂，每个环节都需要最懂运维业务的 SRE 密切参与，与平台、应用研发协同工作，最终各环节联合在一起形成了一套完整的运维体系，在这样的思路下开放技术也会越来越重要。</p><p>平台研发、SRE、应用研发等多种角色协同编写的代码是一种数据的沉淀，亦是一种业务知识的沉淀，基于这些数据和知识，未来会有更多的可能性。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-走上开源之路">6. 走上开源之路<a class="hash-link" href="#6-走上开源之路" title="Direct link to heading">​</a></h2><p>在历经了一段内部探索之后，我们希望把 KusionStack 开源到技术社区。因为我们意识到自身面对的问题，其他公司、团队其实也同样正在面对。借助开源这件事，我们希望团队的这些工作成果能对更多人有所帮助。</p><p>当然，也受限于自身能力以及精力和资源的投入，我们希望能有更多朋友参与进来，与我们共同去完善 KusionStack，不论你是工作在云原生、运维自动化、编程语言或者是编译器中的哪一个领域，我们都非常期待和欢迎你的加入。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="7-期待与你共成长">7. 期待与你共成长<a class="hash-link" href="#7-期待与你共成长" title="Direct link to heading">​</a></h2><p>这段经历对我来说异常宝贵，不仅仅是在于自身再一次在新的技术领域和蚂蚁的技术升级方面尝试了新的探索并实现了突破，更宝贵的是，自己还拥有了一段与一群人均 95 后的小伙伴一起将想法落地实现的奇幻历程。</p><p>在未来，Kusion 的朋友圈不再局限于蚂蚁内部，面向开源，我们期待着能有更多的社区朋友在 KusionStack 与我们共同成长！</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="参考链接">参考链接<a class="hash-link" href="#参考链接" title="Direct link to heading">​</a></h2><ul><li><a href="https://pdos.csail.mit.edu/6.824/papers/borg.pdf" target="_blank" rel="noopener noreferrer">Large-scale cluster management at Google with Borg (PDF)</a></li><li><a href="https://queue.acm.org/detail.cfm?id=2898444" target="_blank" rel="noopener noreferrer">Borg, Omega, and Kubernetes (PDF)</a></li><li><a href="https://sre.google/workbook/configuration-specifics" target="_blank" rel="noopener noreferrer">Configuration Specifics</a></li></ul>]]></content:encoded>
            <category>KusionStack</category>
            <category>Kusion</category>
        </item>
        <item>
            <title><![CDATA[KusionStack Open Day]]></title>
            <link>https://kusionstack.io/zh-CN/blog/2022-open-day</link>
            <guid>2022-open-day</guid>
            <pubDate>Sat, 28 May 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[2022 年 5 月 28 日，KusionStack Open Day 线上线下视频直播正式宣布 KusionStack 一站式可编程配置技术栈（包含 KCL 配置语言、Kusion 引擎、Konfig 配置大库）开源。]]></description>
            <content:encoded><![CDATA[<p>2022 年 5 月 28 日，KusionStack Open Day 线上线下视频直播正式宣布 KusionStack 一站式可编程配置技术栈（包含 KCL 配置语言、Kusion 引擎、Konfig 配置大库）开源。</p><p><img loading="lazy" src="/zh-CN/assets/images/01-4e482cb7a92685aa88e9eb4c70e59bae.jpg" width="1080" height="575"> </p><p>以上是 KusionStack 杭州团队合影。至此，🎉KusionStack Open Day 圆满结束🎉</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-精彩瞬间">1. 精彩瞬间<a class="hash-link" href="#1-精彩瞬间" title="Direct link to heading">​</a></h2><p>感谢大家抽空参加，我们一起来回顾下👀，本次活动的精彩瞬间吧～👏</p><p><img loading="lazy" src="/zh-CN/assets/images/02-52f93b2bc1eddb898b049dc344c1399d.jpg" width="1080" height="720"></p><p>蚂蚁集团高级技术专家、Kusion 项目发起人及负责人——朵晓东作为整场活动的主持人，带领大家一起回顾了 Kusion 项目演进历程，并宣布了 KusionStack 正式开源的好消息！</p><p><img loading="lazy" src="/zh-CN/assets/images/03-2c9990688b8d85f8ab41a54868c1fe77.jpg" width="1080" height="1011"></p><p>活动开场，Kusion 项目的两位 Sponsor——蚂蚁集团可信原生技术部负责人何征宇和技术风险部负责人陈亮，对 Kusion 从研发至今的发展回顾和未来展望。两位 Sponsor 在视频中给予了 Kusion 项目极大的认可，并为 KusionStack 的开源献上了祝福。</p><p><img loading="lazy" src="/zh-CN/assets/images/04-22bbdda26c3d2efb03eea67d8da5f105.jpg" width="653" height="433"></p><p>Kata 创始人、木兰社区 TOC —— 王旭</p><p>王旭在会上也表达了对 KusionStack 开源的喜悦。他指出：一个项目拿出来开源的好的时机，应当是项目还处在未完全成熟的阶段，这样在开源出来之后，才能够通过开源社区的开发者们，一起推动项目更好的发展，开源不是为了秀肌肉。</p><p>在最后，他再次为 KusionStack 的开源送上了诚挚的祝福。</p><p>接下来是各位嘉宾的精彩分享，一起来回顾一下吧～</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-数字化出海业务的-devops-探索和实践">2. 《数字化出海业务的 DevOps 探索和实践》<a class="hash-link" href="#2-数字化出海业务的-devops-探索和实践" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/05-efd253a99042daa806a533e592d54206.png" width="1031" height="644"></p><p>开场演讲的众安国际科技 Engineering 负责人李晓蕾（Sherry Lee），介绍了众安国际 DevOps 在支持数字化出海业务过程遇到难点和对应的解决之道。</p><p>她从数字化出海对 DevOps 带来的挑战、众安国际 DevOps 遇到的难点和解决方案以及 DevOps 具体实践案例分享三个方面展开了宝贵的经验分享。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/0-Sherry-Lee-%E6%95%B0%E5%AD%97%E5%8C%96%E5%87%BA%E6%B5%B7%E4%B8%9A%E5%8A%A1%E7%9A%84DevOps%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%AE%9E%E8%B7%B5.pdf" target="_blank" rel="noopener noreferrer">数字化出海业务的DevOps探索和实践</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1hr4y1x72a" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1hr4y1x72a</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=tYDw__lBcYM" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=tYDw__lBcYM</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-蚂蚁集团规模化-devops-的代际演进探索">3. 《蚂蚁集团规模化 DevOps 的代际演进探索》<a class="hash-link" href="#3-蚂蚁集团规模化-devops-的代际演进探索" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/06-d00776afb137ffb78200c1f2a9e110b1.jpg" width="1080" height="720"></p><p>蚂蚁集团高级技术专家、Kusion 项目发起人及负责人——朵晓东分享了 Kusion 的项目背景和发展进程，同时他宣布 Kusion 正式开源，并分享了开源计划。</p><p>目前，基于 Kusion 的新一代 PaaS 体系已逐步应用在蚂蚁众多内外部场景，在多种运维场景覆盖、规模化协同效率提升、多主体/站点交付运维、技术创新运维效率提升等多方面体系出显著的优势和价值。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/1-%E6%9C%B5%E6%99%93%E4%B8%9C-%E8%9A%82%E8%9A%81%E9%9B%86%E5%9B%A2%E8%A7%84%E6%A8%A1%E5%8C%96DevOps%E4%BB%A3%E9%99%85%E6%BC%94%E8%BF%9B%E6%8E%A2%E7%B4%A2.pdf" target="_blank" rel="noopener noreferrer">蚂蚁集团规模化 DevOps 代际演进探索</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1WZ4y147pC" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1WZ4y147pC</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=T6NKkb1L1eM" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=T6NKkb1L1eM</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-kcl-配置策略语言">4. 《KCL 配置策略语言》<a class="hash-link" href="#4-kcl-配置策略语言" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/07-f9f31715a2bb4bb27220743a833b187e.jpg" width="922" height="546"></p><p>蚂蚁集团高级研发工程师徐鹏飞介绍了 KCL 的相关核心特性，分享了 KCL 技术栈的思路、架构、关键技术，并展开讲述了 KCL 的在蚂蚁内部多场景的实践经历。</p><p>KCL 帮助不同角色的用户以简单、可扩展、稳定、高效、分而治之的方式完成开发和运维任务，同时支持与自动化系统集成，实现极致的执行效率。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/2-%E5%BE%90%E9%B9%8F%E9%A3%9E-KCL%E9%85%8D%E7%BD%AE%E7%AD%96%E7%95%A5%E8%AF%AD%E8%A8%80.pdf" target="_blank" rel="noopener noreferrer">KCL配置策略语言</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1bv4y1w7ke" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1bv4y1w7ke</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=mUFFri_eRAQ" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=mUFFri_eRAQ</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-中场休息">5. 中场休息<a class="hash-link" href="#5-中场休息" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/08-fe322d918ee01bb65bb1e59eec1d7b70.jpg" width="1080" height="720"> </p><p>中场休息期间，“开源老兵”、Go 语言大佬——柴树杉老师浅谈了他参与 KusionStack 的心路历程和个人收获，并表达了能有更多伙伴参与到 KusionStack 的开源共建的希望。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-kusion-模型库和工具链的实践探索和总结">6. 《Kusion 模型库和工具链的实践探索和总结》<a class="hash-link" href="#6-kusion-模型库和工具链的实践探索和总结" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/09-33ea5d03f2565c2cfaa7c5b16b7b1055.jpg" width="1828" height="1032"> </p><p>蚂蚁集团高级研发工程师杨英明以实际的案例介绍了如何通过 KCL 抽象 Kusion 模型库，以及如何结合 Kusion 工具链一站式的完成配置代码的编写和生效，同时总结分享了通过这套模式进行实际交付的经验和建议。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/3-%E6%9D%A8%E8%8B%B1%E6%98%8E-Kusion%E6%A8%A1%E5%9E%8B%E5%BA%93%E5%92%8C%E5%B7%A5%E5%85%B7%E9%93%BE%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%8E%A2%E7%B4%A2%E5%92%8C%E6%80%BB%E7%BB%93.pdf" target="_blank" rel="noopener noreferrer">Kusion模型库和工具链的实践探索和总结</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1Vr4y1x7Ty" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1Vr4y1x7Ty</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=HDUm_KrunLY" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=HDUm_KrunLY</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="7-kusion-在蚂蚁的规模化实践">7. 《Kusion 在蚂蚁的规模化实践》<a class="hash-link" href="#7-kusion-在蚂蚁的规模化实践" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/10-673f24b95d86ef0d9c9f5ed923999ed0.jpg" width="1582" height="896"> </p><p>最后，蚂蚁集团技术专家史贵明和蚂蚁集团高级运维工程师李治玮共同带来了 Kusion 在蚂蚁的规模化实践的分享。史贵明主要从 PaaS 配置管理的系统架构角度讲述了蚂蚁目前的多云配置管理能力，李治玮则从 SRE 视角下，分享了使用 KCL 解决多种复杂基础设施的交付效率问题和价值。</p><ul><li>PDF: <a href="https://github.com/KusionStack/community/blob/main/2022/open-day/4-%E8%8E%AB%E5%9F%8E-%E5%8D%8A%E5%BA%AD-Kusion%E5%9C%A8%E8%9A%82%E8%9A%81%E7%9A%84%E8%A7%84%E6%A8%A1%E5%8C%96%E6%8E%A2%E7%B4%A2%E5%AE%9E%E8%B7%B5.pdf" target="_blank" rel="noopener noreferrer">Kusion在蚂蚁的规模化探索实践</a></li><li>Video(Bilibili): <a href="https://www.bilibili.com/video/BV1xB4y1X7sv" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1xB4y1X7sv</a></li><li>Video(YouTube): <a href="https://www.youtube.com/watch?v=F9lZEU5GNYE" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=F9lZEU5GNYE</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="8-未来展望">8. 未来展望<a class="hash-link" href="#8-未来展望" title="Direct link to heading">​</a></h2><p>开源并不代表 KusionStack 已经完成，相反我们还有很多需要完善和改进的地方，同时开源社区和文化也对文档和代码提出了更高的要求。这只是一个开始，希望更多从事相关领域的同学能够参与共建，为国内的云原生、DSL 等新兴领域贡献力量。</p><p>最后，感谢大家的参与🙏</p>]]></content:encoded>
            <category>KusionStack</category>
            <category>Kusion</category>
            <category>KCL</category>
            <category>KCLVM</category>
        </item>
        <item>
            <title><![CDATA[KCL云原生配置策略语言]]></title>
            <link>https://kusionstack.io/zh-CN/blog/2021-kcl-intro</link>
            <guid>2021-kcl-intro</guid>
            <pubDate>Tue, 03 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[介绍KCL云原生配置语言在蚂蚁的诞生背景、语言特性、实践探索和未来的发展思考。]]></description>
            <content:encoded><![CDATA[<p>介绍KCL云原生配置语言在蚂蚁的诞生背景、语言特性、实践探索和未来的发展思考。</p><ul><li>简介：<a href="https://giac.msup.com.cn/course?id=15307" target="_blank" rel="noopener noreferrer">https://giac.msup.com.cn/course?id=15307</a></li><li>内容：<a href="https://segmentfault.com/a/1190000040455559" target="_blank" rel="noopener noreferrer">https://segmentfault.com/a/1190000040455559</a></li><li><a href="https://gw.alipayobjects.com/os/bmw-prod/2cb0c283-5f24-485e-b635-b6efac887eba.pdf" target="_blank" rel="noopener noreferrer">PDF下载</a></li></ul><p><a href="https://gw.alipayobjects.com/os/bmw-prod/2cb0c283-5f24-485e-b635-b6efac887eba.pdf" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="KCL云原生配置策略语言" src="/zh-CN/assets/images/talk-cover-7bff56b2738955e27cf16a5ebb1a4506.png" width="1820" height="1366"></a></p>]]></content:encoded>
            <category>kcl</category>
        </item>
        <item>
            <title><![CDATA[云原生开放运维体系探索实践]]></title>
            <link>https://kusionstack.io/zh-CN/blog/2021-kusion-intro</link>
            <guid>2021-kusion-intro</guid>
            <pubDate>Tue, 18 May 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[本文是云原生开放协同技术探索与实践一阶段的总结和综述。]]></description>
            <content:encoded><![CDATA[<p><em>本文是云原生开放协同技术探索与实践一阶段的总结和综述。</em></p><p>蚂蚁基础技术在过去3年多以来持续、深入推进全面的云原生化技术演进，我们将在线、离线计算资源装进了一台计算机，将服务体系通过 mesh 的思路和技术手段进行了下沉解耦，可以说比较充分的拥抱了云原生技术，并获取了其带来的技术红利。</p><p>当完成了资源、服务的云原生化，我们发现在云原生基础能力之上的运维体系与云原生技术开放、共享的思路有较大的距离，在技术体系上也与云原生技术声明式、白盒化的思路相悖，同时由于缺少匹配的技术支撑，历史包袱等问题也成为了云原生运维难以真正代际演进的障碍。今天我要介绍的就是蚂蚁在这样的背景下在云原生运维方向进行的技术探索和实践。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-规模化云原生运维探索">1. 规模化云原生运维探索<a class="hash-link" href="#1-规模化云原生运维探索" title="Direct link to heading">​</a></h2><p>我们先来回顾一下在蚂蚁真实的实践方式和面对的问题。首先，我们来看看蚂蚁践行多年的经典运维中台，这类运维平台一般包括了控制器、业务模型、编排引擎、原子任务及管道，在蚂蚁这样的平台是一系列服务的集合，他们较好的满足了集中式、标准化、低变更频率的应用发布及运维需求。但这种模式在实践中也存在着明显的不足。</p><p>首先对于非标准应用、应用个性化需求、高成本需求、非紧急需求、技改类需求，往往无法较好的满足。在蚂蚁的实践中，非标运维需求、对核心应用模型及运维模型冲击较大的高成本改造需求、大量基础能力或运维功能的透出需求等长期无法得到较好的满足，需求往往是合理的，是难以获得足够的优先级执行落地。在研发阶段，运维平台长期积累了高复杂度的业务逻辑，修改测试涉及跨系统的长改造链路，同时基础能力的透出、运维能力的产品化依赖前端、服务端研发资源。这些问题使得运维平台研发日渐吃力，特别是在产品 GUI、业务模型、编排引擎等变更热点上，受限于扩展机制能力不足，内部实践中甚至出现过线上不断修改代码、发布服务以满足需求的情况。平台上线后，统一的质保和线上全链路功能验证同样面对较大的压力。对于最终的使用者，命令式按钮背后的黑盒计算透明度低，审计难，结果难预测，同时激情操作、操作界面不熟悉等问题也一直影响着线上的稳定性。这些问题长期存在，我们寄希望于代际的技术演进来解决这些问题。</p><p><img loading="lazy" src="/zh-CN/assets/images/01-79bb6cdf65fde251f50d5226ab5c2529.png" width="1758" height="716"></p><p>当云原生基础服务逐渐稳定后，对于自身场景不在运维平台管理范围内的应用，研发同学自发的借助云原生社区工具链解决问题。基于 Kubernetes 生态高度开放、高度可配置的特点，研发者可以自助、灵活、透明的声明式应用运行、运维需求，以应用粒度完成发布、运维操作。</p><p>用户通过 kustomize 等社区技术缩短了对接基础设施的路径，并通过如 velocity 等文本模板技术部分解决了静态 YAML 文件在较多变量时维度爆炸的问题，解决了默认值设定的问题，同时通过 code review 的方式进行多因子变更及评审。由于 Kubernetes 及其生态提供了面向资源、服务、运维、安全的横向能力，使得这种简单的方式可有很好的普遍性和适用性，通过对不同的 Kubernetes 集群 “播放” 这些数据即可完成对基础设施的变更，本质上是一种声明数据的流转。面向 git 仓库的研发方式和 gitops 流程支持对运维产品研发资源的诉求较低，往往可以比较简单的搭建起来，不强依赖产品研发资源投入。相比经典运维中台，这些好处清晰明确，但从工程视角缺点也非常明显。</p><p><img loading="lazy" src="/zh-CN/assets/images/02-22efc6ce991ff8f5069ed14ce217bd95.png" width="1536" height="798"></p><p>首先 Kubernetes API 的设计较为复杂，仅是 Kubernetes 原生提供的 low level API 就暴露了 500 多种模型，2000 多字段，场景上几乎涵盖了基础设施应用层的方方面面，即使是专业同学也很难理解所有细节。其次这种方式的工程化程度很低，违反 DRY 原则，违反各团队职责能力高内聚低耦合的原则，即使在有一定的工具支持的情况下，在内部的典型案例中一个多应用的 infra 项目仍然维护了多达 5 万多行 YAML，同时由于团队边界造成的多个割裂的平台，用户需在多个平台间切换，每个平台的操作方式各异，加上跳板机黑屏命令，完成一次完整的发布需要 2 天时间。</p><p>由于低工程化程度的问题，各团队间协同依赖人肉拉群同步，最终 YAML 由多个团队定义的部分组合而成，其中一大部分属于 Kubernetes 及运维平台团队的定义，这些内容需要持续跟踪同步避免腐化，长期维护成本高。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-kusion-云原生开放协同技术栈">2. KUSION: 云原生开放协同技术栈<a class="hash-link" href="#2-kusion-云原生开放协同技术栈" title="Direct link to heading">​</a></h2><p>以上两种模式各有利弊，优势和问题都比较清晰。那么能不能既要也要呢，能不能在继承经典运维平台优势的情况下，充分利用云原生技术带来的红利，打造一个开放、透明、可协同的运维体系？</p><p>带着这样的问题，我们进行了探索和实践，并创建了基于基础设施代码化思路的云原生可编程技术栈 Kusion。</p><p>大家都知道 Kubernetes 提供了声明式的 low level API，提倡其上生态能力通过 CRD 扩展的方式定义并提供服务，整个生态遵循统一的 API 规范约束，复用 API 技术和工具。Kubernetes API 规范提倡 low level API 对象松耦合、可复用，以支持 high level API 由 low level API “组合” 而成。Kubernetes 自身提供了利于开源传播的极简方案，并不包括 API 之上的技术和方案。</p><p>回到云原生技术的本源，我们回看了 Kubernetes 前身 Borg 的应用技术生态。如下图示，在 BorgMaster 之上，Borg 团队研发了 Borg 接入三件套，即 BCL（Borg Configuration Language），Command-line tools，以及相应的 web service。用户可以通过 BCL 声明式编写需求，通过 Command-line tools 将 BCL 文件执行到 Borg 集群，并通过 web GUI 视图查看任务细节。经过大量的调研，我们了解到 Google 内部的运维能力及产品生态、质量技术生态都依赖这三件套构建而成，在内部也进行了多年的迭代演进。</p><p><img loading="lazy" src="/zh-CN/assets/images/03-5eca61ccc90511012f710d0f986f412d.png" width="290" height="278"></p><p>这给了我们启发，今天我们有了容器技术、服务体系，有了大量用户和差异化的需求，有了一定数量的自动化运维平台，我们希望能通过云原生专用的语言和工具来链接 Kubernetes 生态、各个运维平台以及大量的用户，通过唯一事实定义消除运维平台孤岛，完成云原生基础设施在应用、运维层面的代际演进，达到 “Fusion on Kubernetes” 的目标。</p><p>带着这样的目标，我们持续地进行做技术探索和实践，目前已经形成了 Kusion 技术栈，并在蚂蚁的生产实践中进行应用。</p><p><img loading="lazy" src="/zh-CN/assets/images/04-2fb71395a1c3d40287ee376b83a8d81d.png" width="1306" height="1078"></p><p>Kusion 技术栈基于这样的基础能力而工作，包括如下组成部分：</p><ul><li>云原生配置策略专用语言 KCL (Kusion Configuration Language)</li><li>KCL 解释器及其 Plugin 扩展机制</li><li>KCL 研发工具集: Lint, Format, Doc-Gen，IDE Plugin(IDEA, VsCode)</li><li>Kusion Kubernetes 生态工具: OpenAPI-tool, KusionCtl(Srv)</li><li>Konfig 配置代码库，其中包括平台侧及用户侧代码</li><li>OCMP (Open CloudNative Management Practice) 实践说明书</li></ul><p><img loading="lazy" src="/zh-CN/assets/images/05-d5a6d789ec8b357c181433c90cae65d5.png" width="2362" height="1160"></p><p>Kusion 工作在基础设施之上，作为抽象及管理层的技术支撑服务上层应用。不同角色的用户协同使用 Kubernetes 生态提供的横向能力，通过声明式、意图导向的定义方式使用基础设施，在场景上支持典型的云原生场景，也服务了一些经典运维场景，完成了一阶段的建设工作。目前接入 Kusion 的产品包括 IaC 发布、运维产品 InfraForm、建站产品 SiteBuilder、快恢平台等。通过将 Kusion 集成在自动化系统中，我们尽可能的调和黑盒命令式自动化系统与开放声明式配置系统，使其发挥各自的优势。</p><p><img loading="lazy" src="/zh-CN/assets/images/06-db08a31e60ea42621297d1b6e86b506f.png" width="710" height="542"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-集成--落地">3. 集成 &amp; 落地<a class="hash-link" href="#3-集成--落地" title="Direct link to heading">​</a></h2><p>新的技术体系首先面临着落地的问题，我们先来看看 Kusion 在集成落地方面的思考和做法。</p><p>从整体思路上，我们从经典运维系统中的变更热点业务层、编排层着手，以 KCL 声明式配置块的方式外置编写对应逻辑，并被控制器自动化集成。</p><p>这种思路是有迹可循的，我们来看看同行的经验，以雷神山医院的建设现场为例，我们可以看到现场大量的组件是预制品，经过了测试、验证、交付后由现场的塔吊负责组装。这些组件需要良好的品控，需要内置水管、电线等“能力”，否则即使组装也无法有效工作，同时需要给业务侧一定的自定义配置空间，还要易于组装及自动化以提升现场装配效率。实际上我们面对的大规模运维活动与这样的现场有类似之处，现代基建的高效手段非常值得我们学习借鉴。</p><p><img loading="lazy" src="/zh-CN/assets/images/07-a8b81a2606564bb234a8d54d0d3f58f1.png" width="956" height="666"></p><p>对应我们的实际场景，我们基于 KCL 的工作方式需要满足以下要求：</p><ul><li><strong>可分工、可协同：</strong>组件制作、验收、交付、使用可以基于角色需要合理分工协同，满足软件供应链的需求</li><li><strong>外置、预制的组件：</strong>组件独立于自动化系统存在，预制的组件在交付前需要经过充分的测试验证</li><li><strong>内置资源、服务、身份等要素：</strong>组件仅向用户暴露有效的业务信息，同时内置云原生、可信的逻辑</li><li><strong>易于业务定义：</strong>组件需要提供一定的自定义配置能力</li><li><strong>易于自动化：</strong>支持自动化组装，自动化对组件进行“增删改查”</li></ul><p>接下来，我们来看看基于 Kusion 工作的典型流程，此处有一定的抽象和简化。</p><p>前文提到 Kubernetes API 提供了基于 OpenAPI 的 low level API 及扩展机制，基于高内聚、低耦合、易复用、易组装的原则设计，以 Resource、Custome Resource 的方式存在。此外，Kubernetes API 提供了大量的命令以操作容器、Pod 等资源。对于 SDN、Mesh，或是其他的能力扩展都是基于这样的整体约束和方式，大都提供了资源定义或命令操作。</p><p>基于这样的基础，在蚂蚁的实践中我们将整体的工作流程分为 4 个步骤：</p><ul><li><strong>代码化:</strong> 对于资源定义，基于 OpenAPI Model/CRD 定义生成 KCL 结构体；对于命令操作，编写对应的声明式 KCL 结构体。这些结构体对应到平台侧原子能力定义。</li><li><strong>抽象化:</strong> 平台侧 PaaS 平台同学基于这些原子声明式编写抽象、组装，并定义出面向用户的前端结构体，从功能场景上涵盖了 AppConfiguration, Action / Ops, Locality / Topology, SA / RBAC, Node / Quota 等场景，并提供了简化编写的 Template 集合。以 AppConfiguration 为例，我们提供了SigmaAppConfiguration、SigmaJobConfiguration 分别对应于服务型和任务型应用定义，此外针对 SOFA 应用的特征提供了 SofaAppConfiguration。这些前端结构体作为 Kusion Models 的“接口层”存在，受限于业务进度等原因各场景积累的水位不同，仍需要长期的积累打磨。</li><li><strong>配置化:</strong> 应用侧研发或 SRE 同学基于这些前端结构体描述应用需求。用户可以通过结构体声明的方式为应用定义配置基线及不同环境的配置。在大部分情况下，用户仅需要进行结构体声明，即一些 key-value 对。对于有复杂需求的场景，用户可以进行逻辑编写或通过继承结构体的方式组织代码逻辑</li><li><strong>自动化:</strong> 当应用侧配置完成后，实际上已经定义好了可用的“组件”，具备了自动化的条件。平台侧控制器可以通过 KCL CLI 或 GPL binding API 完成编译、执行、输出、代码修改、元素查询等自动化集成工作，用户则可以通过 KusionCtl 工具执行 KCL 代码映射执行到 Kubernetes 集群。</li></ul><p><img loading="lazy" src="/zh-CN/assets/images/08-44399e28b0f5b79daa744587a0856234.png" width="2356" height="1154"></p><p>通过这样统一的工作流程，我们轻量级地完成了对 Kubernetes 生态大量基础能力的透出，基于原子能力声明式地封装、抽象出面向应用的配置、运维能力，并完成了一定场景的落地应用。Kusion 提供了研发工具协助使用者完成其工作。我们可以对平台侧、用户侧分层协同模式下的实践做进一步的探讨。平台侧同学抽象并定义出前端结构体，例如 SofaAppConfiguration ，其中定义了业务镜像、所需资源、config、secrect、sidecar、LB、DNS、副本数、逻辑资源池、发布策略、是否超卖、是否访问公网等等。</p><p>前端结构体无法独立工作，实际上存在着与前端结构体对应的后端结构体，后端对前端透明，前-后端结构体分离解耦。后端结构体在运行时将前端结构体产生的数据“翻译”成对应的 low level API ，这种反向依赖的方式依赖于 KCL 语言能力。</p><p>从工程角度看平台侧同学实际上完成了一次轻量级、声明式的应用级 API 定义。这种前后端分离的设计有诸多好处。首先应用侧使用的前端结构体可以保持简单干净、业务导向、实现细节无关；其次可以通过编译时指向不同的后端文件动态切换到不同的后端结构体实现，以完成平台版本切换、实现切换等目的；最后这样分离的做法可以在统一模式的前提下保证充分的灵活性，例如平台可以通过 kcl base.k prod.k backend.k 多文件编译完成一次包含基线、环境配置、后端结构体的组合编译。事实上，我们可以将所有场景规约为 kcl user_0.k … user_n.k platform_0.k … platform_n.k 的范式，其中 user.k 代表用户侧代码，platform.k 代表平台侧代码。我们从另一个角度来看多团队协同的方式。由各团队自下而上定义平台能力及约束，并完成应用级的配置基线及配置环境特征，完成最后一公里的定义。</p><p><img loading="lazy" src="/zh-CN/assets/images/09-7a74ef990a90b1319880938d66e8a31c.png" width="2150" height="1126"></p><p>在理清工作流程后，我们来看 KCL 通过 Konfig 大库落地的实践。我们在 Konfig 代码仓库中定义了平台侧及用户侧的代码空间，通过统一配置代码库完成对代码的共享和复用，保证了对整体基础设施代码定义的可见性。在用户侧，通过 project、stack、component(对应蚂蚁内部应用) 三级目录的方式组织代码。以 cloudmesh 为例，在 tnt/middleware/cloudmesh 的 project 目录下含多个 stack，如 dev、prod，每个 stack 中含多个 component。代码在这三个维度得以隔离，并共享上下文。</p><p><img loading="lazy" src="/zh-CN/assets/images/10-459ff8a8b60ed9a731b3c969bec76a2f.png" width="1264" height="852"></p><p>在代码质保方面，我们通过单元测试、集成测试等手段保证对平台侧、用户侧代码的质量，我们正在引入代码扫描、配置回放、配置校验、dry-run 等验证手段保证代码变更的可靠性。在研发方面，我们通过主干开发、分支发布的方式保证不同应用并行研发的前提下尽可能不产生代码腐化的情况，并通过 tag 保护稳定分支。</p><p><img loading="lazy" src="/zh-CN/assets/images/11-9288b20e7e93ea9514ce49323ae1732c.png" width="2008" height="398"></p><p>在 IaC 产品落地场景中，通过标准化的结构体、代码版本化、多环境代码隔离、CI pipeline 等手段管理基础设施描述代码，通过代码变更的静态、动态 diff、模拟、异常提示、风险管控接入保证基础设施变更可控，通过代码 Pull Request 做变更审计及对变更人员的追踪。下图以业务发布场景为例展示了关键步骤，在业务代码通过质保流程并完成镜像构建后，CI 流程控制器通过 KCL API 对 Konfig 仓库中对应 KCL 文件中的 image 字段进行自动更新，并发起 Pull Request，由此触发发布流程。</p><p>IaC 提供了编译测试、live-diff、dry-run、风险管控接入等验证方式，并支持执行过程的可视化，产品基于 KCL 语言能力及工具建设，尽可能的减少业务定制。整个流程以 Konfig 代码的自动修改为起点，平台方、应用方、SRE 基于代码协同，通过产品界面进行线上发布，支持分批分步、回滚等运维能力。Konfig 中的代码“组件”可以被多个场景集成使用，例如此处被发布控制器集成的组件还可以被建站控制器集成，控制器只需关注自动化逻辑，无需关心被集成组件的内部细节。以文章开头的典型建站场景为例，在接入 Kusion 后，用户侧配置代码减少到 5.5%，用户面对的 4 个平台通过接入统一代码库而消减，在无其他异常的情况下交付时间从 2 天下降到 2 小时。</p><p><img loading="lazy" src="/zh-CN/assets/images/12-195ae3b012287a996749e577b427df62.png" width="2218" height="846"></p><p>我们再来看更加动态性的大规模快速恢复场景。快恢平台在接到监控告警输入后决策产生异常容器 hostname 列表，并需要对容器进行重启等恢复操作。</p><p>我们通过 KCL 编写声明式的应用恢复运维代码，其中通过 KCL Plugin 扩展完成对在线 CMDB 的查询，将 hostname 列表转换为多集群 Pod 列表，并声明式定义 Pod 恢复操作。快恢平台执行 KusionCtl run AppRecovery.k 完成跨多集群的 Pod 恢复操作。通过这样的方式，快恢控制器无需理解容器恢复细节、Kubernetes 多集群映射执行细节等，可以更专注于自身异常判断及决策逻辑。</p><p><img loading="lazy" src="/zh-CN/assets/images/13-06bb5d00c402a5b01df6e1b91f4547d3.png" width="2236" height="1154"></p><p>在项目落地过程中，我们也发现到了不少因为进度等原因造成的平台侧设计问题。例如平台侧操作定义不够规范，应用依赖等共性定义过于分散等问题，这需要我们在后续的落地过程中持续去沉淀提高。开放配置给了用户更大的灵活性和空间，但相比黑盒的方式需要更多的安全性保障。在开放协同推进的同时，可信原生技术部在并行推进云原生可信平台的建设，可信平台通过将身份与 Kubernetes 技术紧密结合提供相比社区方案能力更强的技术支撑。</p><p>举个例子，通过开放配置我们是不是可以通过 mount 证书的方式使得不可信不安全的服务获得访问目标服务的权限从而获取到关键数据？事实上在没有身份传递及高水位 Pod 安全保障的前提下这是完全可能。通过可信平台对 PSP（Pod Security Policy）、服务验证、服务鉴权等场景的加固，使得我们可以按需增强关键链路的安全策略。相比与社区方案，可信平台定义了更完整的 spiffe 身份标识，并使得身份作用于资源、网络、服务的各个环节，可以说可信是开放的必要前提。同时可信提供的鉴权能力、隔离能力也需要被用户使用，将原子能力封装并在应用配置层面透出依赖于 Kusion 的推进，使得接入 Kusion 的应用可以更简单的使用可信能力。可以说开放协同技术栈与可信平台是能力正交，相辅相成的云原生应用层技术。</p><p><img loading="lazy" src="/zh-CN/assets/images/14-adbbc347994d20aae4ce0916250c76f1.png" width="960" height="441"></p><p><strong>最后，我们对集成落地做一个小结：</strong></p><p>平台侧编写 80% 内容，通过面向应用的前端结构体提供规范的配置块，再通过后端结构体定义屏蔽 low level API 资源及操作，最终通过这样的方式描述应用对 workload、编排、运维等方面的需求，重点在于可以定义什么、默认有什么及约束集合，并通过 Konfig 仓库共享复用。平台侧趋向引擎化，专注自动化控制逻辑，由 KCL 代码作为扩展技术外置编写业务逻辑。我们希望面对复杂的运维业务诉求，平台侧控制器逐步演进到低频变更，甚至零变更。</p><p>应用侧输入 20% 内容，以平台侧前端结构体为界面声明应用侧诉求，重点在于要什么、要做什么，所写即所得。应用侧通过面向多项目、多租户、多环境、多应用的代码工程结构组织代码，通过 Pull Request 发起变更，通过 CICD pipeline 完成白盒化的线上变更。同时，应用侧有对单应用编译、测试、验证、模拟的自由度，在充分验证后交付使用；对多应用可通过 KCL 语言能力按需灵活组合。将大规模的复杂问题拆分缩小到应用粒度，得到充分验证后按需合并，本质上是一种分治思路的实践。针对蚂蚁的实际情况，我们通过 KusionCtl 工具支持研发测试环境的执行及可视化，通过 InfraForm 产品、SiteBuilder 产品等推动线上的部署过程。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-协同配置问题模型">4. 协同配置问题模型<a class="hash-link" href="#4-协同配置问题模型" title="Direct link to heading">​</a></h2><p>理解了落地思路和场景实践方式，我们将进一步下钻拆解具体的协同场景，同时分析 KCL 语言在配置场景的设计和应用。</p><p>我们先来看平台侧编写轻量级应用级 API 的一些要点。平台侧同学可以通过单继承的方式扩展结构体，通过 mixin 机制定义结构体内属性的依赖关系及值内容，通过结构体内顺序无关的编写方式完成声明式的结构体定义，此外还支持如逻辑判断、默认值等常用功能。</p><p>对于声明式与命令式的差异做简单的分析，我们以斐波那契数列为例，可以把一组声明式代码看作一个方程组，方程式的编写顺序本质上不影响求解，而“求解”的过程由 KCL 解释器完成，这样可以避免大量命令式拼装过程及顺序判断代码，对于存在复杂依赖的结构体而言优化尤为明显。</p><p>对于复杂结构，命令式拼装的写法多出一倍以上的代码量，补丁代码使得结果难以预测，同时需要考虑执行顺序问题，特别是在模块化过程中调整存在依赖的模块顺序非常繁琐且易出错。对于各种配套能力，我们通过 mixin 机制编写，并通过 mixin 声明的方式“混入”到不同的结构体中。</p><p><img loading="lazy" src="/zh-CN/assets/images/15-3a5a0768baaa395c6c70fd5a59d0dd9d.png" width="2236" height="1082"></p><p>对于平台侧来说，稳定性保证尤为重要。</p><p>当配置数据量逐步增大时，良构类型是保证编译时问题发现的有效手段，KCL spec 包括了完备的类型系统设计，我们正在实践静态类型检查和推导，逐步增强类型的完备性。</p><p>同时 KCL 引入了多种不可变手段，支持用户按需定义结构体内属性的不可变性。通过这两种基础而重要的技术手段使得大量违反编写约束的情况可以在编译时被检查发现。</p><p><img loading="lazy" src="/zh-CN/assets/images/16-36f723413c8f7d55169049c9c2f76dec.png" width="2014" height="1076"></p><p>对于业务向的内容，KCL 支持通过结构体内置的校验规则及单元测试的方式支持。以下图所示代码为例，我们在 AppBase 中定义对 containerPort、services、volumes 的校验规则，同时在 MyProdApp 中定义叠加的环境相关的校验规则。目前校验规则在运行时执行判断，我们正在尝试通过编译时的静态分析对规则进行判断从而发现问题。</p><p><img loading="lazy" src="/zh-CN/assets/images/17-4c4e4ab7f1e7871e3ebe721aac60d1aa.png" width="1636" height="826"></p><p>此外对于平台侧来说，升级推进是必须面对的问题。我们首先需要考虑最坏情况，即提供给用户的前端结构体需要做不兼容的调整，按照新增配置项并下线老配置项的思路，我们需要对待下线字段进行禁用，并以合理的方式告知用户。</p><p>当平台自身出现不兼容更新时问题相似，只是需要平台侧后端结构体进行调整，应用侧用户不直接感知。KCL 针对这类问题提供了字段禁用的功能，使用被禁用字段将在编译阶段通过警告或错误的方式提示，编译错误将 block 编译，从而迫使用户在编译阶段进行修改，避免将问题带入运行时造成影响。</p><p>对于兼容的平台侧调整，通常在后端结构体修改导入的原子定义文件即可。对于 KCL 解释器自身的变化，我们通过单元测试、集成测试、模糊测试等进行验证，对于 plugin 的变更通过 plugin 自身的测试验证。KCL 解释器及 plugin 的变化通过需要 Konfig 代码库的 UT、IT 进行测试验证，保障已有代码正常工作。在经过测试验证后，发起 Pull Request 通过 code review 评审。</p><p><img loading="lazy" src="/zh-CN/assets/images/18-98948ced448014abb15351065bc9b0c3.png" width="1258" height="784"></p><p>我们再来简单梳理应用侧协同的场景。假设存在基线配置及生产环境配置，在我们的实践中存在三种典型场景。</p><p>第一种场景中，基线与生产配置中各定义了同名配置的一部分，由 KCL 自动合并生成最终配置块，这适用于对称配置的场景非常有效，如果出现冲突则会进行冲突报错。</p><p>第二种场景中，我们希望在生产配置中覆盖基线配置中的一些配置项，类似 Kustomize 的 overlay 覆盖功能，事实上这是大多数熟悉 Kubernetes 使用者的诉求。</p><p>对于第三种场景，编写者希望配置块全局唯一，不能进行任何形式的修改，若出现同名配置则会在编译阶段报错。在真实的场景中，基线与各环境配置可由研发与 SRE 配合完成，也可以由 Dev 独立完成，Kusion 本身不限制使用者职能。</p><p><img loading="lazy" src="/zh-CN/assets/images/19-e783dd554ad98c689adabf10c0c4e79a.png" width="2352" height="1040"></p><p>通过场景分析我们对 KCL 有了初步的了解，我们以编程语言的理论、技术，云原生应用场景三方面为输入设计 KCL，我们希望通过简单有效的技术手段支撑平台侧、应用侧完成基础设施描述，将问题尽可能暴露在 KCL 编译、测试阶段，以减少线上运行时的问题频次。此外我们提供了便利的语言能力和工具帮助不同的使用群体更高效的完成其工作，并通过工程化的方式组织、共享代码，对接 Kubernetes API 生态。</p><p><img loading="lazy" src="/zh-CN/assets/images/20-91999d9afe2c59f42c861f4b41cc9eed.png" width="2252" height="1152"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-抽象模型">5. 抽象模型<a class="hash-link" href="#5-抽象模型" title="Direct link to heading">​</a></h2><p>通过对 Kusion 落地集成、协同编程场景的分析，我们了解到 Kusion 技术的组成场景及使用方式。我们再来看看 Kusion 关键抽象模型。</p><p>我们先来看 KCL 代码的抽象模型。以下图为例，首先 KCL 代码在编译过程中形成两张有向无环图，分别对应结构体内部声明代码及结构体使用声明。编译过程可以简单分为展开、合并、代换三步。通过这样的计算过程，在编译时完成了大部分代换运算，最终运行时进行少量计算即可得到最终的解。在编译过程中，我们同步进行类型检查和值的检查，他们的区别是类型检查是做泛化，取偏序上确界，值检查是做特化，取偏序下确界。 &gt;</p><p><img loading="lazy" src="/zh-CN/assets/images/21-e6504c15635929d9abf205ca849bc1da.png" width="1836" height="1146"></p><p>对于 KCLVM 的解释器，我们采用了标准的分层解耦的设计方式，由 parser、compiler、VM 三部分组成。我们希望尽可能的在编译时完成工作，例如图的展开、代换，类型的检查、推导等，这样可以保持 VM 部分尽可能简单。后续我们将在 KCLVM compiler 中支持对 WASM 中间表示的编译支持。此外我们通过 plugin 机制支持对 VM 运行时能力的扩展，并考虑了对 LSP Server 的支持以降低 IDE、编辑器支持成本。</p><p><img loading="lazy" src="/zh-CN/assets/images/22-a36339df8cee3c1d33beea9f655c2848.png" width="1278" height="588"></p><p>在工程化方面，我们通过 project、stack、component 三级方式组织 KCL 代码。当代码映射到 Kubernetes 集群时，Kusion 支持两种映射方式。</p><p>第一种方式支持将 stack 映射为 namespace，component 在 namespace 内存在，即 stack 内共享资源配额，component 间通过 SDN 及 Mesh 能力做隔离，这是社区比较常见的一种实践方式。</p><p>第二种方式将 component 映射为 namespace，stack 通过 label 标识，通过 SA 管理权限，资源配额定义在 component 维度，component 间通过 namespace 的隔离能力做隔离，这是蚂蚁目前线上环境的实践方式。无论如何映射，用户无需感知物理集群对接及切换细节。此外，KCL 代码中资源定义都可以通过唯一的资源 ID 定位，这也是对代码进行“增删改查”的基础。</p><p><img loading="lazy" src="/zh-CN/assets/images/23-d86a1f8e507b90c43a7d28133575af6b.png" width="1628" height="1052"></p><p>为了支持上述的隔离及映射逻辑，我们提供了 KusionCtl 工具帮助用户完成项目结构初始化、Kubernetes 集群映射、执行状态跟踪及展示、Identity 权限集成等常用功能。用户可以通过 KusionCtl 完成研发、测试环境的执行和验证工作。</p><p><img loading="lazy" src="/zh-CN/assets/images/24-d351dc8a680bb652a1996162397cab27.png" width="968" height="1158"></p><p>对于线上环境，我们更推荐使用基于 Kusion 的运维产品进行变更操作。我们希望通过 KCL 代码开放、透明、声明式、意图导向、分层解耦的定义基础设施，本质上是面向数据及其约束的一种协同工作，变更是一种数据的流动。我们通过前置的预编译、计算、验证，最终将数据交付到各环境的运行时，相比于经典命令式系统中计算逻辑流动的方式，可以最大程度避免复杂命令式计算造成的运行时数据错误，特别是当计算逻辑发生变更时，这种运行时计算错误的结果通常都是一次线上故障。</p><p>最后，我们来看一种 Kusion 思路的技术架构，我们仍然以控制器、业务层、编排层、任务及管道的分层逻辑来看。自下而上的，由 Kubernetes API Server 收敛了管道并提供了原生资源定义，并通过 CRD &amp; Operator 进行扩展提供稳定的原子任务定义。从我个人的角度看，Operator 如其名约“操作员”，重复着接收订单、执行操作的简单循环，订单未完成则持续操作。</p><p>Operator 应尽可能保持简单，避免复杂的业务逻辑拆解、控制逻辑、状态机，同时避免因为微小的差异创建新的 Operator 或通过 Operator 做单纯的数据、YAML 转换。Operator 作为收敛基础设施原子能力的存在，应尽量内聚、稳定。在业务层、编排层，我们通过 KCL 代码在 Konfig 仓库中编写，并结合 GitOps 支持应用粒度的变更、编译、测试、验证。控制器层高度引擎化，聚焦自动化逻辑，根据业务场景需要定制控制器及 GUI 产品界面。应用的配置代码“组件”由多个控制器共享复用，例如建站、发布、部分运维都将依赖应用 AppConfiguration 配置代码块。</p><p><img loading="lazy" src="/zh-CN/assets/images/25-bb23856fe0a42527bee9c3bc0626872d.png" width="1546" height="1160"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-总结--展望">6. 总结 &amp; 展望<a class="hash-link" href="#6-总结--展望" title="Direct link to heading">​</a></h2><p>最后，我们对开放协同技术工作做一个总结。</p><p>我们常说 Kubernetes 是云计算的 Linux/Unix，相比于 Unix 丰富的外围配套生态，Kubernetes 在配套技术能力上还有很长的路径。对比于使用便利的 Shell、Tools，我们还缺少一种符合 Kubernetes 声明式、开放、共享设计理念的语言及工具，Kusion 希望能在这一领域有所帮助，提升基础设施的开放程度及使用效率，易于共享、协同，提升稳定性，简化云原生技术设施的接入方式。</p><p><img loading="lazy" src="/zh-CN/assets/images/26-d18442d2d0183911f70f69bda0e16ad4.png" width="1004" height="530"></p><p>我们的探索和实践仍然在一个初级阶段，我们希望通过 Kusion 的技术和服务能力在运维、可信、云原生架构演进方面起到积极的作用。</p><p>我们希望推进真正的基础设施代码化，促成跨团队的 DevOps，成为持续部署与运维的技术支撑。在可信方面，策略及代码、可信集成、标准化的支撑是我们后续的工作重点之一，特别是与策略引擎的结合，是开放可信技术能力的关键步骤。</p><p>在云原生架构方面，我们将持续推进架构现代化的演进，通过技术手段支持更多上层自动化产品业务的快速创新，同时通过统一的流程、企业级的技术能力支持服务好基础设施应用场景。</p><p><img loading="lazy" src="/zh-CN/assets/images/27-f506606d2e5ab41b4c930f2a2ebe5287.png" width="1564" height="834"></p><p>纵观历史，技术总是朝着提高整体社会协作效能演进。 Kusion 带来的云原生开放协同无疑是这条朴素规律再次发挥效力的注脚。</p>]]></content:encoded>
            <category>kusion</category>
        </item>
    </channel>
</rss>